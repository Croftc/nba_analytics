{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x20563c8ee90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.utils import resample\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "from ccb_model import BootstrapCalibratedClassifier\n",
    "\n",
    "# =============================================\n",
    "# Imports and Initial Setup\n",
    "# =============================================\n",
    "# =============================================\n",
    "# Imports and Initial Setup\n",
    "# =============================================\n",
    "\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from itertools import combinations, product\n",
    "\n",
    "# For RL agent and environment\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from sb3_contrib import MaskablePPO\n",
    "from sb3_contrib.common.maskable.policies import MaskableMultiInputActorCriticPolicy\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "import tensorboard\n",
    "# List of columns where NaNs are allowed\n",
    "allowed_nan_columns = [\n",
    "    'TOTAL_LINE_MOVEMENT_3',\n",
    "    'SPREAD_LINE_MOVEMENT_3',\n",
    "    'SPREAD_LINE_MOVEMENT_1',\n",
    "    'TOTAL_LINE_MOVEMENT_2',\n",
    "    'SPREAD_LINE_MOVEMENT_2',\n",
    "    'TOTAL_LINE_MOVEMENT_1',\n",
    "    'HOME TEAM WIN%',\n",
    "    'CREW',\n",
    "    'FOUL DIFFERENTIAL (Against Road Team) - (Against Home Team)',\n",
    "    'HOME TEAM POINTS DIFFERENTIAL',\n",
    "    'MAIN REF',\n",
    "    'FOUL% AGAINST HOME TEAMS',\n",
    "    'TOTAL POINTS PER GAME',\n",
    "    'CALLED FOULS PER GAME',\n",
    "    'FOUL% AGAINST ROAD TEAMS'\n",
    "]\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_profit(odds, size):\n",
    "    if odds > 0:\n",
    "        profit = (odds / 100) * size\n",
    "    else:\n",
    "        profit = (100 / -(odds + 0.0000001)) * size\n",
    "    return profit\n",
    "\n",
    "def kelly_criterion(bankroll, probability, odds, temper=1):\n",
    "    \"\"\"\n",
    "    Calculate the optimal bet size using the Kelly Criterion.\n",
    "\n",
    "    :param bankroll: Total amount of money you have to bet with.\n",
    "    :param probability: The probability of the bet winning (from 0 to 1).\n",
    "    :param odds: The odds being offered on the bet (in decimal format).\n",
    "    :return: The recommended bet size according to the Kelly Criterion.\n",
    "    \"\"\"\n",
    "    # Convert American odds to decimal if necessary\n",
    "    if odds > 0:\n",
    "        odds = (odds / 100) + 1\n",
    "    elif odds < 0:\n",
    "        odds = (100 / -odds) + 1\n",
    "\n",
    "    # Calculate the Kelly bet fraction\n",
    "    b = odds - 1  # Decimal odds minus 1\n",
    "    q = 1 - probability  # Probability of losing\n",
    "    kelly_fraction = (b * probability - q) / b\n",
    "\n",
    "    # Calculate the recommended bet\n",
    "    recommended_bet = (temper * kelly_fraction) * bankroll\n",
    "\n",
    "    return recommended_bet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get today's date\n",
    "from datetime import datetime\n",
    "today = datetime.today().strftime('%Y-%m-%d')\n",
    "yesterday = (datetime.today() - pd.DateOffset(1)).strftime('%Y-%m-%d')\n",
    "two_days_ago = (datetime.today() - pd.DateOffset(2)).strftime('%Y-%m-%d')\n",
    "three_days_ago = (datetime.today() - pd.DateOffset(3)).strftime('%Y-%m-%d')\n",
    "df = pd.read_csv(f'2024_2025_nba_team_full_{three_days_ago}.csv')\n",
    "\n",
    "# Dropping rows with NaN values, except in specified columns\n",
    "#df = df.dropna(subset=[col for col in df.columns if col not in allowed_nan_columns])\n",
    "\n",
    "# flip true and false in ml_result column\n",
    "df['ml_result'] = df['ml_result'].apply(lambda x: True if x == False else False)\n",
    "\n",
    "# convert categorical columns\n",
    "df['MAIN REF'] = df['MAIN REF'].astype('category')\n",
    "df['CREW'] = df['CREW'].astype('category')\n",
    "df['TEAM_REST_DAYS'] = df['TEAM_REST_DAYS'].astype('category')\n",
    "df['TEAM'] = df['TEAM'].astype('category')\n",
    "df['Opponent'] = df['Opponent'].astype('category')\n",
    "\n",
    "# convert venue to binary\n",
    "df['VENUE'] = (df['VENUE'] == 'H')*1\n",
    "\n",
    "# convert date to datetime\n",
    "df['DATE'] = pd.to_datetime(df['DATE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train / test / val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['spread_result', 'ml_result', 'total_result', 'q3_result', 'DATE', 'POSS', 'OEFF', 'DEFF', 'PACE', 'PTS'])\n",
    "X_rl = df.drop(columns=['q3_result', 'POSS', 'OEFF', 'DEFF', 'PACE', 'PTS'])\n",
    "y_ml = df['ml_result']\n",
    "y_spread = df['spread_result']\n",
    "y_q3 = df['q3_result']\n",
    "y_total = df['total_result']\n",
    "\n",
    "X_train_ml, X_test_ml, y_train_ml, y_test_ml = train_test_split(X, y_ml, test_size=0.2, random_state=42)\n",
    "X_train_ml, X_val_ml, y_train_ml, y_val_ml = train_test_split(X_train_ml, y_train_ml, test_size=0.1, random_state=41)\n",
    "\n",
    "X_train_spread, X_test_spread, y_train_spread, y_test_spread = train_test_split(X, y_spread, test_size=0.2, random_state=42)\n",
    "X_train_spread, X_val_spread, y_train_spread, y_val_spread = train_test_split(X_train_spread, y_train_spread, test_size=0.1, random_state=41)\n",
    "\n",
    "X_train_q3, X_test_q3, y_train_q3, y_test_q3 = train_test_split(X, y_q3, test_size=0.2, random_state=42)\n",
    "X_train_q3, X_val_q3, y_train_q3, y_val_q3 = train_test_split(X_train_q3, y_train_q3, test_size=0.1, random_state=41)\n",
    "\n",
    "X_train_total, X_test_total, y_train_total, y_test_total = train_test_split(X, y_total, test_size=0.2, random_state=42)\n",
    "X_train_total, X_val_total, y_train_total, y_val_total = train_test_split(X_train_total, y_train_total, test_size=0.1, random_state=41)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models loaded from spread_model\n",
      "Models loaded from ml_model\n",
      "Models loaded from total_model\n"
     ]
    }
   ],
   "source": [
    "spread_model = BootstrapCalibratedClassifier(n_bootstrap_samples=5)\n",
    "spread_model.load_model('spread_model')\n",
    "\n",
    "ml_model = BootstrapCalibratedClassifier(n_bootstrap_samples=5)\n",
    "ml_model.load_model('ml_model')\n",
    "\n",
    "total_model = BootstrapCalibratedClassifier(n_bootstrap_samples=5)\n",
    "total_model.load_model('total_model')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### backtesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['spread_result', 'ml_result', 'total_result', 'GAME-ID', 'DATE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def odds_to_implied_prob(odds):\n",
    "    # Convert American odds to implied probability\n",
    "    if odds > 0:\n",
    "        return 100 / (odds + 100)\n",
    "    else:\n",
    "        return abs(odds) / (abs(odds) + 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a column in X_train_ml that is the predicted probability of the model\n",
    "df_rl = X_rl = df.drop(columns=['q3_result', 'POSS', 'OEFF', 'DEFF', 'PACE', 'PTS']).copy(deep=True)\n",
    "ml_preds = ml_model.predict_proba_raw(df_rl.drop(drop_cols, axis=1))[:,:,1].T\n",
    "spread_preds = spread_model.predict_proba_raw(df_rl.drop(drop_cols, axis=1))[:,:,1].T\n",
    "total_preds = total_model.predict_proba_raw(df_rl.drop(drop_cols, axis=1))[:,:,1].T\n",
    "# X_rl['spread_prob'] = spread_model.predict_proba(df_rl.drop(drop_cols, axis=1))[:, 1]\n",
    "# X_rl['total_prob'] = total_model.predict_proba(df_rl.drop(drop_cols, axis=1))[:, 1]\n",
    "X_rl[['ml_prob1', 'ml_prob2','ml_prob3','ml_prob4','ml_prob5',]] = ml_preds\n",
    "X_rl[['spread_prob1', 'spread_prob2','spread_prob3','spread_prob4','spread_prob5',]] = spread_preds\n",
    "X_rl[['total_prob1', 'total_prob2','total_prob3','total_prob4','total_prob5',]] = total_preds\n",
    "\n",
    "# add a mean and variance column for each model\n",
    "X_rl['ml_mean'] = X_rl[['ml_prob1', 'ml_prob2','ml_prob3','ml_prob4','ml_prob5',]].mean(axis=1)\n",
    "X_rl['ml_var'] = X_rl[['ml_prob1', 'ml_prob2','ml_prob3','ml_prob4','ml_prob5',]].var(axis=1)\n",
    "X_rl['spread_mean'] = X_rl[['spread_prob1', 'spread_prob2','spread_prob3','spread_prob4','spread_prob5',]].mean(axis=1)\n",
    "X_rl['spread_var'] = X_rl[['spread_prob1', 'spread_prob2','spread_prob3','spread_prob4','spread_prob5',]].var(axis=1)\n",
    "X_rl['total_mean'] = X_rl[['total_prob1', 'total_prob2','total_prob3','total_prob4','total_prob5',]].mean(axis=1)\n",
    "X_rl['total_var'] = X_rl[['total_prob1', 'total_prob2','total_prob3','total_prob4','total_prob5',]].var(axis=1)\n",
    "\n",
    "# create implied odds columns\n",
    "X_rl['SPREAD_LINE'] = -110\n",
    "X_rl['TOTAL_LINE'] = -110\n",
    "X_rl['implied_ml_odds'] = X_rl['MONEYLINE'].apply(odds_to_implied_prob)\n",
    "X_rl['implied_spread_odds'] = X_rl['SPREAD_LINE'].apply(odds_to_implied_prob)\n",
    "X_rl['implied_total_odds'] = X_rl['TOTAL_LINE'].apply(odds_to_implied_prob)\n",
    "\n",
    "# create a value column as the difference between the implied odds and the model odds mean\n",
    "X_rl['ml_value'] = X_rl['ml_mean'] - X_rl['implied_ml_odds']\n",
    "X_rl['spread_value'] = X_rl['spread_mean'] - X_rl['implied_spread_odds']\n",
    "X_rl['total_value'] = X_rl['total_mean'] - X_rl['implied_total_odds']\n",
    "\n",
    "# flip true and false in ml_result column\n",
    "X_rl['ml_result'] = X_rl['ml_result'].apply(lambda x: True if x == False else False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = X_rl[['GAME-ID', 'DATE', 'TEAM', 'Opponent',\n",
    "               'ml_mean', 'ml_var', 'MONEYLINE', 'ml_result',\n",
    "                'spread_mean','spread_var', 'SPREAD_LINE','spread_result',\n",
    "                'implied_ml_odds', 'implied_spread_odds','ml_value', 'spread_value', \n",
    "                'ml_prob1', 'ml_prob2', 'ml_prob3', 'ml_prob4', 'ml_prob5',\n",
    "                'spread_prob1', 'spread_prob2', 'spread_prob3', 'spread_prob4', 'spread_prob5']].copy(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single bet env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import logging\n",
    "from collections import deque\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt  # Import for plotting\n",
    "import wandb  # Import wandb\n",
    "\n",
    "# Initialize wandb\n",
    "wandb.init(project='parlays_dqn_agent')\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 256)  # Adjust layer sizes as needed\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "def odds_to_implied_prob(odds):\n",
    "    # Convert American odds to implied probability\n",
    "    if odds > 0:\n",
    "        return 100 / (odds + 100)\n",
    "    else:\n",
    "        return abs(odds) / (abs(odds) + 100)\n",
    "\n",
    "def get_state(games_data, bankroll, max_games=15):\n",
    "    # Initialize state with zeros and pad for games < max_games\n",
    "    state = np.zeros((max_games, 2, 20))  # [Games, Teams, Features]\n",
    "    \n",
    "    game_ids = games_data['GAME-ID'].unique()\n",
    "    for i, game_id in enumerate(game_ids):\n",
    "        if i >= max_games:\n",
    "            break  # Only consider up to max_games\n",
    "        \n",
    "        game_df = games_data[games_data['GAME-ID'] == game_id]\n",
    "        teams = game_df['TEAM'].unique()\n",
    "        \n",
    "        for j, team in enumerate(teams):\n",
    "            team_data = game_df[game_df['TEAM'] == team].iloc[0]\n",
    "            # Predicted probabilities\n",
    "            ml_prob = team_data['ml_mean']\n",
    "            spread_prob = team_data['spread_mean']\n",
    "            # Offered odds\n",
    "            ml_odds = team_data['MONEYLINE']\n",
    "            spread_odds = team_data['SPREAD_LINE']\n",
    "            # Other features\n",
    "            prob1 = team_data['ml_prob1']\n",
    "            prob2 = team_data['ml_prob2']\n",
    "            prob3 = team_data['ml_prob3']\n",
    "            prob4 = team_data['ml_prob4']\n",
    "            prob5 = team_data['ml_prob5']\n",
    "\n",
    "            spread_prob1 = team_data['spread_prob1']\n",
    "            spread_prob2 = team_data['spread_prob2']\n",
    "            spread_prob3 = team_data['spread_prob3']\n",
    "            spread_prob4 = team_data['spread_prob4']\n",
    "            spread_prob5 = team_data['spread_prob5']\n",
    "\n",
    "            implied_ml_odds = team_data['implied_ml_odds']\n",
    "            implied_spread_odds = team_data['implied_spread_odds']\n",
    "            ml_value = team_data['ml_value']\n",
    "            spread_value = team_data['spread_value']\n",
    "\n",
    "            ml_var = team_data['ml_var']\n",
    "            spread_var = team_data['spread_var']\n",
    "            \n",
    "            # Implied probabilities\n",
    "            ml_imp_prob = odds_to_implied_prob(ml_odds)\n",
    "            spread_imp_prob = odds_to_implied_prob(spread_odds)\n",
    "            # Value indicators\n",
    "            ml_value = ml_prob - ml_imp_prob\n",
    "            spread_value = spread_prob - spread_imp_prob\n",
    "            # Assign features\n",
    "            state[i, j, :] = [ml_prob, ml_odds, ml_value, spread_prob, spread_odds, spread_value, prob1, prob2, prob3, prob4, prob5,\n",
    "                              spread_prob1, spread_prob2, spread_prob3, spread_prob4, spread_prob5,\n",
    "                              implied_ml_odds, implied_spread_odds, ml_var, spread_var]\n",
    "    \n",
    "    state = state.flatten()\n",
    "    state = np.append(state, bankroll / 10000)  # Normalize bankroll\n",
    "    return state\n",
    "\n",
    "def generate_actions(games_data, stake_sizes=[0.01, 0.02, 0.03, 0.04, 0.05]):\n",
    "    actions = []\n",
    "    game_ids = games_data['GAME-ID'].unique()\n",
    "    for i, game_id in enumerate(game_ids):\n",
    "        game_df = games_data[games_data['GAME-ID'] == game_id]\n",
    "        teams = game_df['TEAM'].unique()\n",
    "        for team in teams:\n",
    "            for market in ['ML', 'Spread']:\n",
    "                for stake in stake_sizes:\n",
    "                    action = {\n",
    "                        'game_index': i,\n",
    "                        'team': team,\n",
    "                        'market': market,\n",
    "                        'stake': stake\n",
    "                    }\n",
    "                    actions.append(action)\n",
    "    return actions\n",
    "\n",
    "class ParlaysDQNAgent:\n",
    "    def __init__(self, state_size, action_size, batch_size=64, gamma=0.99,\n",
    "                 epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995,\n",
    "                 learning_rate=0.001, target_update=10, memory_size=10000,\n",
    "                 logging_level=logging.INFO):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.epsilon = epsilon_start  # Exploration rate\n",
    "        self.epsilon_min = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.learning_rate = learning_rate\n",
    "        self.target_update = target_update  # Episodes between target network updates\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "        self.bankroll = 10000  # Starting bankroll\n",
    "        \n",
    "        # Neural networks\n",
    "        self.policy_net = DQN(state_size, action_size)\n",
    "        self.target_net = DQN(state_size, action_size)\n",
    "        self.update_target_net()\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.learning_rate)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        \n",
    "        # Set up logging\n",
    "        self.logger = logging.getLogger('ParlaysDQNAgent')\n",
    "        self.logger.setLevel(logging_level)\n",
    "        handler = logging.StreamHandler()\n",
    "        handler.setLevel(logging_level)\n",
    "        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "        handler.setFormatter(formatter)\n",
    "        if not self.logger.handlers:\n",
    "            self.logger.addHandler(handler)\n",
    "        self.logger.propagate = False  # Prevent duplicate logs\n",
    "        \n",
    "    def update_target_net(self):\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        \n",
    "    def act(self, state, available_actions):\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            # Exploration: Choose a random action from available actions\n",
    "            action_idx = random.choice(available_actions)\n",
    "        else:\n",
    "            # Exploitation: Choose the best action based on policy_net predictions\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                q_values = self.policy_net(state_tensor)\n",
    "            q_values = q_values.detach().numpy()[0]\n",
    "            # Mask unavailable actions\n",
    "            masked_q_values = np.full(self.action_size, -np.inf)\n",
    "            masked_q_values[available_actions] = q_values[available_actions]\n",
    "            action_idx = np.argmax(masked_q_values)\n",
    "        return action_idx\n",
    "    \n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return  # Not enough samples to train\n",
    "        \n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        \n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.LongTensor(actions).unsqueeze(1)\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        next_states = torch.FloatTensor(next_states)\n",
    "        dones = torch.BoolTensor(dones)\n",
    "        \n",
    "        # Current Q values\n",
    "        q_values = self.policy_net(states).gather(1, actions).squeeze()\n",
    "        \n",
    "        # Target Q values\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_net(next_states).max(1)[0]\n",
    "        target_q_values = rewards + self.gamma * next_q_values * (~dones)\n",
    "        \n",
    "        # Loss\n",
    "        loss = self.criterion(q_values, target_q_values)\n",
    "        \n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Log training loss to wandb\n",
    "        wandb.log({'training_loss': loss.item()})\n",
    "        \n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "        \n",
    "    def save_model(self, filepath):\n",
    "        torch.save(self.policy_net.state_dict(), filepath)\n",
    "        \n",
    "    def load_model(self, filepath):\n",
    "        self.policy_net.load_state_dict(torch.load(filepath))\n",
    "        self.update_target_net()\n",
    "        \n",
    "    def american_to_decimal_odds(self, odds):\n",
    "        # Convert American odds to decimal odds\n",
    "        if odds > 0:\n",
    "            return (odds / 100) + 1\n",
    "        else:\n",
    "            return (100 / abs(odds)) + 1\n",
    "\n",
    "def calculate_reward(bet_result):\n",
    "    # Reward is solely based on actual profit or loss\n",
    "    reward = bet_result\n",
    "    return reward\n",
    "\n",
    "def train_agent(agent, df, episodes=1000, max_days_per_episode=None):\n",
    "    grouped = df.groupby('DATE')\n",
    "    dates = list(grouped.groups.keys())\n",
    "    max_games = 15\n",
    "    \n",
    "    bankroll_history = []  # List to store bankroll at the end of each episode\n",
    "    \n",
    "    # Log hyperparameters to wandb\n",
    "    wandb.config.update({\n",
    "        'episodes': episodes,\n",
    "        'max_days_per_episode': max_days_per_episode,\n",
    "        'batch_size': agent.batch_size,\n",
    "        'gamma': agent.gamma,\n",
    "        'epsilon_start': agent.epsilon,\n",
    "        'epsilon_end': agent.epsilon_min,\n",
    "        'epsilon_decay': agent.epsilon_decay,\n",
    "        'learning_rate': agent.learning_rate,\n",
    "        'target_update': agent.target_update,\n",
    "        'memory_size': len(agent.memory),\n",
    "    })\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        agent.logger.info(f\"Starting episode {episode + 1}/{episodes} with epsilon: {agent.epsilon:.4f}\")\n",
    "        if max_days_per_episode:\n",
    "            selected_dates = random.sample(dates, min(max_days_per_episode, len(dates)))\n",
    "        else:\n",
    "            selected_dates = dates  # Use all dates\n",
    "        \n",
    "        # Reset bankroll at the start of each episode\n",
    "        agent.bankroll = 10000\n",
    "        \n",
    "        total_episode_reward = 0  # Track total reward for the episode\n",
    "        \n",
    "        for date in selected_dates:\n",
    "            games_data = grouped.get_group(date)\n",
    "            unique_game_ids = games_data['GAME-ID'].unique()\n",
    "            if len(unique_game_ids) < 1:\n",
    "                continue  # Skip if no games\n",
    "            \n",
    "            state = get_state(games_data, agent.bankroll, max_games)\n",
    "            actions = generate_actions(games_data)\n",
    "            action_size = len(actions)\n",
    "            available_actions = list(range(action_size))\n",
    "            \n",
    "            # Take action\n",
    "            action_idx = agent.act(state, available_actions)\n",
    "            action = actions[action_idx]\n",
    "            stake_amount = action['stake'] * agent.bankroll\n",
    "            team = action['team']\n",
    "            market = action['market']\n",
    "            game_index = action['game_index']\n",
    "            \n",
    "            # Get game data\n",
    "            game_ids = games_data['GAME-ID'].unique()\n",
    "            game_id = game_ids[game_index]\n",
    "            game_df = games_data[games_data['GAME-ID'] == game_id]\n",
    "            team_data = game_df[game_df['TEAM'] == team].iloc[0]\n",
    "            \n",
    "            # Calculate expected value\n",
    "            if market == 'ML':\n",
    "                predicted_prob = team_data['ml_mean']\n",
    "                odds = team_data['MONEYLINE']\n",
    "                result = team_data['ml_result']\n",
    "            else:\n",
    "                predicted_prob = team_data['spread_mean']\n",
    "                odds = team_data['SPREAD_LINE']\n",
    "                result = team_data['spread_result']\n",
    "            \n",
    "            implied_prob = odds_to_implied_prob(odds)\n",
    "            value = predicted_prob - implied_prob\n",
    "            expected_value = value  # Expected value per unit stake\n",
    "            \n",
    "            # Calculate bet result\n",
    "            if result:\n",
    "                decimal_odds = agent.american_to_decimal_odds(odds)\n",
    "                bet_result = stake_amount * (decimal_odds - 1)\n",
    "            else:\n",
    "                bet_result = -stake_amount\n",
    "            \n",
    "            # Update bankroll\n",
    "            agent.bankroll += bet_result\n",
    "            \n",
    "            # Next state\n",
    "            next_state = get_state(games_data, agent.bankroll, max_games)\n",
    "            done = False  # In this simplified example, episodes don't have a terminal state\n",
    "            \n",
    "            # Calculate reward\n",
    "            reward = calculate_reward(bet_result)\n",
    "            total_episode_reward += reward  # Accumulate reward\n",
    "            \n",
    "            # Remember and train\n",
    "            agent.remember(state, action_idx, reward, next_state, done)\n",
    "            agent.replay()\n",
    "            \n",
    "            state = next_state  # Update state for next step\n",
    "            \n",
    "            # Log per-action metrics to wandb\n",
    "            wandb.log({\n",
    "                'date': date,\n",
    "                'episode': episode + 1,\n",
    "                'epsilon': agent.epsilon,\n",
    "                'bankroll': agent.bankroll,\n",
    "                'bet_result': bet_result,\n",
    "                'stake_amount': stake_amount,\n",
    "                'action_idx': action_idx,\n",
    "                'reward': reward,\n",
    "            })\n",
    "            \n",
    "        # Decay epsilon\n",
    "        agent.decay_epsilon()\n",
    "        \n",
    "        # Update target network\n",
    "        if episode % agent.target_update == 0:\n",
    "            agent.update_target_net()\n",
    "        \n",
    "        agent.logger.info(f\"Ending episode {episode + 1}/{episodes} with bankroll: {agent.bankroll}\\n\")\n",
    "        \n",
    "        bankroll_history.append(agent.bankroll)  # Record bankroll at the end of the episode\n",
    "        \n",
    "        # Log per-episode metrics to wandb\n",
    "        wandb.log({\n",
    "            'episode': episode + 1,\n",
    "            'epsilon': agent.epsilon,\n",
    "            'episode_reward': total_episode_reward,\n",
    "            'bankroll': agent.bankroll,\n",
    "        })\n",
    "        \n",
    "    # Save the trained model\n",
    "    agent.save_model('parlays_dqn_model.pth')\n",
    "    \n",
    "    return bankroll_history  # Return the bankroll history for analysis\n",
    "\n",
    "\n",
    "# Prepare agent\n",
    "state_size = (15 * 2 * 20) + 1  # 15 games, 2 teams, 4 features per team, plus bankroll\n",
    "max_games = 15\n",
    "\n",
    "# Generate actions for all possible games to get consistent action size\n",
    "actions = []\n",
    "for _ in range(max_games):\n",
    "    for team in ['TeamA', 'TeamB']:\n",
    "        for market in ['ML', 'Spread']:\n",
    "            for stake in [0.01, 0.02, 0.03, 0.04, 0.05]:\n",
    "                action = {\n",
    "                    'game_index': _,\n",
    "                    'team': team,\n",
    "                    'market': market,\n",
    "                    'stake': stake\n",
    "                }\n",
    "                actions.append(action)\n",
    "action_size = len(actions)\n",
    "\n",
    "agent = ParlaysDQNAgent(\n",
    "    state_size=state_size,\n",
    "    action_size=action_size,\n",
    "    batch_size=64,\n",
    "    gamma=0.99,\n",
    "    epsilon_start=1,\n",
    "    epsilon_end=0.01,\n",
    "    epsilon_decay=0.995,\n",
    "    learning_rate=0.001,\n",
    "    target_update=10,\n",
    "    memory_size=10000,\n",
    "    logging_level=logging.INFO\n",
    ")\n",
    "\n",
    "# Train agent and get bankroll history\n",
    "bankroll_history = train_agent(agent, train, episodes=500)\n",
    "\n",
    "# Initial and final bankrolls\n",
    "initial_bankroll = 10000  # Starting bankroll\n",
    "final_bankroll = bankroll_history[-1]\n",
    "total_profit = final_bankroll - initial_bankroll\n",
    "roi = (total_profit / initial_bankroll) * 100\n",
    "\n",
    "print(f\"Total Profit: ${total_profit:.2f}\")\n",
    "print(f\"ROI: {roi:.2f}%\")\n",
    "\n",
    "# Plot bankroll over episodes\n",
    "episodes = range(1, len(bankroll_history) + 1)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(episodes, bankroll_history, marker='o')\n",
    "plt.title('Agent Bankroll Over Episodes')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Bankroll')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Finish the wandb run\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### parlay env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# =============================================\n",
    "# Data Preparation Functions\n",
    "# =============================================\n",
    "\n",
    "def odds_to_implied_prob(odds):\n",
    "    if odds > 0:\n",
    "        return 100 / (odds + 100)\n",
    "    else:\n",
    "        return abs(odds) / (abs(odds) + 100)\n",
    "\n",
    "def get_state(games_data, agent_context, max_games=15):\n",
    "    state_sequence = []\n",
    "    game_ids = games_data['GAME-ID'].unique()\n",
    "    for idx, game_id in enumerate(game_ids):\n",
    "        if idx >= max_games:\n",
    "            break\n",
    "        game_df = games_data[games_data['GAME-ID'] == game_id]\n",
    "        teams = game_df['TEAM'].unique()\n",
    "        if len(teams) < 2:\n",
    "            continue\n",
    "        team_features = []\n",
    "        for team in teams:\n",
    "            team_data = game_df[game_df['TEAM'] == team].iloc[0]\n",
    "            features = [\n",
    "                team_data['spread_mean'],\n",
    "                team_data['SPREAD_LINE'],\n",
    "                team_data['spread_value'],\n",
    "                team_data['spread_prob1'],\n",
    "                team_data['spread_prob2'],\n",
    "                team_data['spread_prob3'],\n",
    "                team_data['spread_prob4'],\n",
    "                team_data['spread_prob5'],\n",
    "                team_data['implied_spread_odds'],\n",
    "                team_data['spread_var']\n",
    "            ]\n",
    "            team_features.append(features)\n",
    "        if len(team_features) == 2:\n",
    "            state_sequence.append(team_features)\n",
    "    agent_features = np.array([\n",
    "        agent_context['bankroll'] / agent_context['initial_bankroll'],\n",
    "        #agent_context['parlays_placed'] / agent_context['max_parlays']\n",
    "    ])\n",
    "    return state_sequence, agent_features\n",
    "\n",
    "def generate_valid_parlays(games_data, parlay_size):\n",
    "    game_picks = []\n",
    "    game_ids = games_data['GAME-ID'].unique()\n",
    "    for idx, game_id in enumerate(game_ids):\n",
    "        game_df = games_data[games_data['GAME-ID'] == game_id]\n",
    "        teams = game_df['TEAM'].unique()\n",
    "        if len(teams) < 2:\n",
    "            continue\n",
    "        team_a = teams[0]\n",
    "        team_b = teams[1]\n",
    "        picks = [\n",
    "            {'game_index': idx, 'game_id': game_id, 'team': team_a},\n",
    "            {'game_index': idx, 'game_id': game_id, 'team': team_b}\n",
    "        ]\n",
    "        game_picks.append(picks)\n",
    "    game_combinations = combinations(game_picks, parlay_size)\n",
    "    all_parlays = []\n",
    "    for game_combo in game_combinations:\n",
    "        parlay_combos = list(product(*game_combo))\n",
    "        all_parlays.extend(parlay_combos)\n",
    "    return all_parlays\n",
    "\n",
    "\n",
    "# =============================================\n",
    "# Action Masking Function\n",
    "# =============================================\n",
    "\n",
    "def get_action_mask(env):\n",
    "    return env.action_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### single bet env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Set a seed for reproducibility\n",
    "SEED = 42\n",
    "\n",
    "def get_state(games_data, agent_context, max_games=15):\n",
    "    state_sequence = []\n",
    "    game_ids = games_data['GAME-ID'].unique()\n",
    "    for idx, game_id in enumerate(game_ids):\n",
    "        if idx >= max_games:\n",
    "            break\n",
    "        game_df = games_data[games_data['GAME-ID'] == game_id]\n",
    "        teams = game_df['TEAM'].unique()\n",
    "        if len(teams) < 2:\n",
    "            continue\n",
    "        team_features = []\n",
    "        for team in teams:\n",
    "            team_data = game_df[game_df['TEAM'] == team].iloc[0]\n",
    "            features = [\n",
    "                team_data['spread_mean'],\n",
    "                team_data['SPREAD_LINE'],\n",
    "                team_data['spread_value'],\n",
    "                team_data['spread_prob1'],\n",
    "                team_data['spread_prob2'],\n",
    "                team_data['spread_prob3'],\n",
    "                team_data['spread_prob4'],\n",
    "                team_data['spread_prob5'],\n",
    "                team_data['implied_spread_odds'],\n",
    "                team_data['spread_var']\n",
    "            ]\n",
    "            team_features.append(features)\n",
    "        if len(team_features) == 2:\n",
    "            state_sequence.append(team_features)\n",
    "    agent_features = np.array([\n",
    "        agent_context['bankroll'] / agent_context['initial_bankroll'],\n",
    "    ])\n",
    "    return state_sequence, agent_features\n",
    "\n",
    "class BettingEnv(gym.Env):\n",
    "    metadata = {'render_modes': ['human']}\n",
    "    \n",
    "    def __init__(self, data, max_games=6, stake_per_bet=100, initial_bankroll=10000):\n",
    "        super(BettingEnv, self).__init__()\n",
    "        self.data = data\n",
    "        self.dates = self.data['DATE'].unique()\n",
    "        self.max_games = max_games\n",
    "        self.stake_per_bet = stake_per_bet\n",
    "        self.initial_bankroll = initial_bankroll\n",
    "        \n",
    "        # Number of features per team (from get_state function)\n",
    "        self.num_features_per_team = 5  # As per the get_state function\n",
    "        self.agent_feature_dim = 1  # Only bankroll ratio as agent feature\n",
    "        \n",
    "        # Define action space: Decide whether to bet on each game (binary decision for each game)\n",
    "        self.action_space = spaces.MultiBinary(self.max_games)\n",
    "        \n",
    "        # Define observation space\n",
    "        self.observation_space = spaces.Dict({\n",
    "            'state_sequence': spaces.Box(\n",
    "                low=-np.inf,\n",
    "                high=np.inf,\n",
    "                shape=(self.max_games, 2, self.num_features_per_team),\n",
    "                dtype=np.float32\n",
    "            ),\n",
    "            'agent_features': spaces.Box(\n",
    "                low=-np.inf,\n",
    "                high=np.inf,\n",
    "                shape=(self.agent_feature_dim,),  # Adjust if agent_features has more dimensions\n",
    "                dtype=np.float32\n",
    "            ),\n",
    "            'action_mask': spaces.MultiBinary(self.max_games)\n",
    "        })\n",
    "        \n",
    "        # Initialize state variables\n",
    "        self.current_date_idx = -1\n",
    "        self.current_date = None\n",
    "        self.games_data = None\n",
    "        self.agent_context = None\n",
    "        self.bets_placed = None\n",
    "        self.total_stake = 0\n",
    "        self.done = False\n",
    "        \n",
    "        self.seed(SEED)\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self, seed=None, options=None):\n",
    "        if seed is not None:\n",
    "            self.seed(seed)\n",
    "        \n",
    "        # Move to the next date\n",
    "        self.current_date_idx = (self.current_date_idx + 1) % len(self.dates)\n",
    "        self.current_date = self.dates[self.current_date_idx]\n",
    "        \n",
    "        # Get games for the current date\n",
    "        self.games_data = self.data[self.data['DATE'] == self.current_date].reset_index(drop=True)\n",
    "        \n",
    "        # Convert 'GAME-ID's to strings\n",
    "        self.games_data['GAME-ID'] = self.games_data['GAME-ID'].astype(str)\n",
    "        \n",
    "        # Initialize agent context\n",
    "        self.agent_context = {\n",
    "            'bankroll': self.initial_bankroll,\n",
    "            'initial_bankroll': self.initial_bankroll,\n",
    "        }\n",
    "        \n",
    "        self.bets_placed = []\n",
    "        self.total_stake = 0\n",
    "        self.done = False\n",
    "        \n",
    "        # Prepare state_sequence and agent_features using get_state\n",
    "        self.state_sequence, self.agent_features = get_state(self.games_data, self.agent_context, max_games=self.max_games)\n",
    "        \n",
    "        # Pad state_sequence if less than max_games\n",
    "        num_games = len(self.state_sequence)\n",
    "        if num_games < self.max_games:\n",
    "            padding = [[[0]*self.num_features_per_team for _ in range(2)] for _ in range(self.max_games - num_games)]\n",
    "            self.state_sequence.extend(padding)\n",
    "        \n",
    "        # Prepare action_mask\n",
    "        self.action_mask = np.ones(self.max_games, dtype=np.int8)\n",
    "        for idx in range(self.max_games):\n",
    "            if idx >= num_games:\n",
    "                self.action_mask[idx] = 0  # No game data for this index\n",
    "            else:\n",
    "                # Check bankroll\n",
    "                if self.agent_context['bankroll'] < self.stake_per_bet:\n",
    "                    self.action_mask[idx] = 0  # Not enough bankroll to bet\n",
    "        \n",
    "        # Prepare observation\n",
    "        observation = self._get_observation()\n",
    "        return observation, {}\n",
    "    \n",
    "    def step(self, action):\n",
    "        if self.done:\n",
    "            raise ValueError(\"Episode has ended. Please reset the environment.\")\n",
    "        \n",
    "        # Ensure action is a binary array of length max_games\n",
    "        if not isinstance(action, np.ndarray):\n",
    "            action = np.array(action)\n",
    "        if action.shape != (self.max_games,):\n",
    "            raise ValueError(f\"Action must be of shape ({self.max_games},), got {action.shape}\")\n",
    "        \n",
    "        # Initialize total profit for this step\n",
    "        total_profit = 0\n",
    "        \n",
    "        # Process each game based on the action\n",
    "        for idx, bet_decision in enumerate(action):\n",
    "            if bet_decision == 1 and self.action_mask[idx] == 1:\n",
    "                if idx >= len(self.games_data):\n",
    "                    continue  # No game data\n",
    "                game = self.games_data.iloc[idx]\n",
    "                # Place the bet\n",
    "                self.bets_placed.append(game)\n",
    "                self.agent_context['bankroll'] -= self.stake_per_bet\n",
    "                self.total_stake += self.stake_per_bet\n",
    "                \n",
    "                # Calculate the result of the bet\n",
    "                profit = self.calculate_bet_reward(game, self.stake_per_bet)\n",
    "                total_profit += profit\n",
    "        \n",
    "        # Update bankroll with profit/loss\n",
    "        self.agent_context['bankroll'] += total_profit\n",
    "        \n",
    "        # Prepare reward\n",
    "        reward = total_profit\n",
    "        \n",
    "        # Prepare next observation\n",
    "        observation = self._get_observation()\n",
    "        \n",
    "        # Set done to True as we simulate one day per episode\n",
    "        self.done = True\n",
    "        \n",
    "        info = {}\n",
    "        \n",
    "        return observation, reward, self.done, False, info\n",
    "    \n",
    "    def _get_observation(self):\n",
    "        observation = {\n",
    "            'state_sequence': np.array(self.state_sequence, dtype=np.float32),\n",
    "            'agent_features': np.array(self.agent_features, dtype=np.float32),\n",
    "            'action_mask': self.action_mask\n",
    "        }\n",
    "        return observation\n",
    "    \n",
    "    def calculate_bet_reward(self, game, stake):\n",
    "        # Determine the outcome of the bet\n",
    "        if game['spread_result']:\n",
    "            odds = game['ODDS']  # Ensure 'ODDS' column exists in your data\n",
    "            decimal_odds = self.american_to_decimal_odds(odds)\n",
    "            profit = stake * (decimal_odds - 1)\n",
    "        else:\n",
    "            profit = -stake\n",
    "        return profit\n",
    "    \n",
    "    def american_to_decimal_odds(self, odds):\n",
    "        # Convert American odds to decimal odds\n",
    "        if odds > 0:\n",
    "            return (odds / 100) + 1\n",
    "        else:\n",
    "            return (100 / abs(odds)) + 1\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        if mode == 'human':\n",
    "            print(f\"Date: {self.current_date}\")\n",
    "            print(f\"Bankroll: {self.agent_context['bankroll']}\")\n",
    "            print(f\"Bets Placed: {len(self.bets_placed)}\")\n",
    "            for bet in self.bets_placed:\n",
    "                print(f\" - Game ID: {bet['GAME-ID']}, Team: {bet['TEAM']}, Spread Result: {bet['spread_result']}\")\n",
    "        else:\n",
    "            super().render(mode=mode)\n",
    "    \n",
    "    def close(self):\n",
    "        pass\n",
    "    \n",
    "    def seed(self, seed=None):\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        # Uncomment the following line if you're using PyTorch\n",
    "        torch.manual_seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (6, 2) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDATE\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mfilter(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;241m4\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(x) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m6\u001b[39m)\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Create the environment\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mBettingEnv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_games\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstake_per_bet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_bankroll\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Define the action mask function\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_action_mask\u001b[39m(env):\n",
      "Cell \u001b[1;32mIn[37], line 82\u001b[0m, in \u001b[0;36mBettingEnv.__init__\u001b[1;34m(self, data, max_games, stake_per_bet, initial_bankroll)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseed(SEED)\n\u001b[1;32m---> 82\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[37], line 128\u001b[0m, in \u001b[0;36mBettingEnv.reset\u001b[1;34m(self, seed, options)\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_mask[idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# Not enough bankroll to bet\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;66;03m# Prepare observation\u001b[39;00m\n\u001b[1;32m--> 128\u001b[0m observation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_observation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m observation, {}\n",
      "Cell \u001b[1;32mIn[37], line 177\u001b[0m, in \u001b[0;36mBettingEnv._get_observation\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_observation\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    176\u001b[0m     observation \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m--> 177\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate_sequence\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    178\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124magent_features\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_features, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32),\n\u001b[0;32m    179\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maction_mask\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_mask\n\u001b[0;32m    180\u001b[0m     }\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m observation\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (6, 2) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "\n",
    "# Set a seed for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Load your dataset\n",
    "# data = pd.read_csv('your_data.csv')  # Replace with your actual data loading method\n",
    "# Ensure that 'data' is a pandas DataFrame with necessary columns\n",
    "\n",
    "# Filter data to have dates with between 4 and 6 games\n",
    "data = data.groupby('DATE').filter(lambda x: 4 <= len(x) <= 6).reset_index(drop=True)\n",
    "\n",
    "# Create the environment\n",
    "env = BettingEnv(data=data, max_games=6, stake_per_bet=100, initial_bankroll=10000)\n",
    "\n",
    "# Define the action mask function\n",
    "def get_action_mask(env):\n",
    "    return env._get_observation()['action_mask']\n",
    "\n",
    "# Wrap the environment with ActionMasker\n",
    "env = ActionMasker(env, get_action_mask)\n",
    "\n",
    "# Define policy_kwargs\n",
    "policy_kwargs = dict(\n",
    "    features_extractor_class=CustomPolicyNetwork,\n",
    "    features_extractor_kwargs=dict(\n",
    "        embedding_dim=128,\n",
    "        num_heads=4,\n",
    "        num_layers=2,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Determine the device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Training on device: {device}\")\n",
    "\n",
    "# Instantiate the Maskable PPO agent\n",
    "model = MaskablePPO(\n",
    "    policy=MaskableMultiInputActorCriticPolicy,\n",
    "    env=env,\n",
    "    learning_rate=0.0003,\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    verbose=1,\n",
    "    tensorboard_log=\"./ppo_betting_tensorboard/\",\n",
    "    seed=SEED,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Train the agent\n",
    "model.learn(total_timesteps=250000)\n",
    "\n",
    "# Save the trained model\n",
    "model.save('ppo_betting_agent')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ActionMasker' object has no attribute 'max_games'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Generate a random valid action\u001b[39;00m\n\u001b[0;32m      6\u001b[0m valid_actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(action_masks \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m----> 7\u001b[0m random_action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_games\u001b[49m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(valid_actions) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m      9\u001b[0m     random_action[valid_actions] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(valid_actions))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ActionMasker' object has no attribute 'max_games'"
     ]
    }
   ],
   "source": [
    "# Test the environment\n",
    "obs, _ = env.reset()\n",
    "action_masks = obs['action_mask']\n",
    "\n",
    "# Generate a random valid action\n",
    "valid_actions = np.where(action_masks == 1)[0]\n",
    "random_action = np.zeros(env.max_games, dtype=int)\n",
    "if len(valid_actions) > 0:\n",
    "    random_action[valid_actions] = np.random.randint(0, 2, size=len(valid_actions))\n",
    "\n",
    "# Step through the environment\n",
    "new_obs, reward, done, _, info = env.step(random_action)\n",
    "\n",
    "# Test the model's forward pass\n",
    "model.policy.to(device)\n",
    "obs_tensor = model.policy.obs_to_tensor(obs)[0]\n",
    "with torch.no_grad():\n",
    "    action, _ = model.policy.predict(obs, deterministic=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### parlays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# Custom Environment Definition\n",
    "# =============================================\n",
    "\n",
    "class BettingEnv(gym.Env):\n",
    "    metadata = {'render_modes': ['human']}\n",
    "\n",
    "    def __init__(self, data, max_parlays=1, parlay_size=2, max_games=6):\n",
    "        super(BettingEnv, self).__init__()\n",
    "        self.data = data\n",
    "        self.dates = self.data['DATE'].unique()\n",
    "        self.max_parlays = max_parlays\n",
    "        self.parlay_size = parlay_size\n",
    "        self.max_games = max_games\n",
    "\n",
    "        # Calculate the maximum possible number of actions\n",
    "        self.max_actions = self.calculate_max_possible_actions(self.max_games, self.parlay_size)\n",
    "        self.action_space = spaces.Discrete(self.max_actions)\n",
    "        self.observation_space = spaces.Dict({\n",
    "            'state_sequence': spaces.Box(\n",
    "                low=-np.inf,\n",
    "                high=np.inf,\n",
    "                shape=(self.max_games, 2, 10),\n",
    "                dtype=np.float32\n",
    "            ),\n",
    "            'agent_features': spaces.Box(\n",
    "                low=-np.inf,\n",
    "                high=np.inf,\n",
    "                shape=(2,),\n",
    "                dtype=np.float32\n",
    "            )\n",
    "            # Removed 'action_mask' from observation_space\n",
    "        })\n",
    "        self.current_date_idx = -1\n",
    "        self.current_date = None\n",
    "        self.games_data = None\n",
    "        self.agent_context = None\n",
    "        self.state_sequence = None\n",
    "        self.agent_features = None\n",
    "        self.parlays = None\n",
    "        self.action_space_size = None\n",
    "        self.action_mask = None\n",
    "        self.parlays_placed = []\n",
    "        self.total_stake = 0\n",
    "        self.step_count = 0\n",
    "        self.done = False\n",
    "        self.max_steps = self.max_parlays\n",
    "        self.seed(SEED)\n",
    "        self.reset()\n",
    "\n",
    "    def calculate_max_possible_actions(self, max_games, parlay_size):\n",
    "        from math import comb\n",
    "        num_combinations = comb(max_games, parlay_size)\n",
    "        num_actions = num_combinations * (2 ** parlay_size)\n",
    "        return num_actions\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        if seed is not None:\n",
    "            self.seed(seed)\n",
    "        self.current_date_idx = (self.current_date_idx + 1) % len(self.dates)\n",
    "        self.current_date = self.dates[self.current_date_idx]\n",
    "        self.games_data = self.data[self.data['DATE'] == self.current_date].reset_index(drop=True)\n",
    "        self.agent_context = {\n",
    "            'bankroll': 10000,\n",
    "            'initial_bankroll': 10000,\n",
    "            'parlays_placed': 0,\n",
    "            'max_parlays': self.max_parlays\n",
    "        }\n",
    "        self.parlays_placed = []\n",
    "        self.total_stake = 0\n",
    "        self.step_count = 0\n",
    "        self.done = False\n",
    "        self.state_sequence, self.agent_features = get_state(self.games_data, self.agent_context, self.max_games)\n",
    "        self.parlays = generate_valid_parlays(self.games_data, self.parlay_size)\n",
    "        if len(self.parlays) > self.max_actions:\n",
    "            self.parlays = random.sample(self.parlays, self.max_actions)\n",
    "        self.action_space_size = len(self.parlays)\n",
    "        self.action_mask = np.zeros(self.max_actions, dtype=bool)\n",
    "        self.action_mask[:self.action_space_size] = True\n",
    "        # Ensure there is at least one valid action\n",
    "        if not self.action_mask.any():\n",
    "            # Handle case when there are no valid actions\n",
    "            raise ValueError(\"No valid actions available at reset.\")\n",
    "        observation = self._get_observation()\n",
    "        return observation, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        self.step_count += 1\n",
    "        if 0 <= action < self.action_space_size and self.action_mask[action]:\n",
    "            selected_parlay = self.parlays[action]\n",
    "            self.parlays_placed.append(selected_parlay)\n",
    "            self.agent_context['parlays_placed'] += 1\n",
    "            stake_per_parlay = 100\n",
    "            self.total_stake += stake_per_parlay\n",
    "            self.agent_context['bankroll'] -= stake_per_parlay\n",
    "            self.agent_features = np.array([\n",
    "                self.agent_context['bankroll'] / self.agent_context['initial_bankroll'],\n",
    "                self.agent_context['parlays_placed'] / self.agent_context['max_parlays']\n",
    "            ])\n",
    "            self.action_mask[action] = False\n",
    "            if not self.action_mask.any() and not self.done:\n",
    "                # Handle the case where no valid actions are left\n",
    "                self.done = True\n",
    "                reward = self.compute_reward()\n",
    "                observation = self._get_observation()\n",
    "                terminated = self.done\n",
    "                truncated = False\n",
    "                return observation, reward, terminated, truncated, {}\n",
    "            if self.agent_context['parlays_placed'] >= self.max_parlays:\n",
    "                self.done = True\n",
    "                reward = self.compute_reward()\n",
    "            else:\n",
    "                reward = 0\n",
    "            observation = self._get_observation()\n",
    "            terminated = self.done\n",
    "            truncated = False\n",
    "            return observation, reward, terminated, truncated, {}\n",
    "        else:\n",
    "            reward = -10\n",
    "            self.done = True\n",
    "            observation = self._get_observation()\n",
    "            terminated = self.done\n",
    "            truncated = False\n",
    "            return observation, reward, terminated, truncated, {}\n",
    "\n",
    "    def _get_padded_state_sequence(self):\n",
    "        state_sequence = self.state_sequence.copy()\n",
    "        num_games = len(state_sequence)\n",
    "        if num_games < self.max_games:\n",
    "            padding = [[[0]*10, [0]*10] for _ in range(self.max_games - num_games)]\n",
    "            state_sequence.extend(padding)\n",
    "        return np.array(state_sequence, dtype=np.float32)\n",
    "\n",
    "    def _get_observation(self):\n",
    "        observation = {\n",
    "            'state_sequence': self._get_padded_state_sequence(),\n",
    "            'agent_features': self.agent_features.astype(np.float32)\n",
    "            # Removed 'action_mask' from the observation\n",
    "        }\n",
    "        return observation\n",
    "\n",
    "    def render(self):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        set_random_seed(seed)\n",
    "\n",
    "    def compute_reward(self):\n",
    "        parlay_results = []\n",
    "        for parlay in self.parlays_placed:\n",
    "            parlay_profit = self.calculate_parlay_reward(parlay, self.games_data, stake=100)\n",
    "            parlay_results.append(parlay_profit)\n",
    "        total_profit = sum(parlay_results)\n",
    "        reward = total_profit\n",
    "        return reward\n",
    "\n",
    "    def calculate_parlay_reward(self, parlay, games_data, stake):\n",
    "        parlay_win = True\n",
    "        parlay_odds = 1.0\n",
    "        for pick in parlay:\n",
    "            game_index = pick['game_index']\n",
    "            team = pick['team']\n",
    "            game_id = pick['game_id']\n",
    "            game_df = games_data[games_data['GAME-ID'] == game_id]\n",
    "            team_data = game_df[game_df['TEAM'] == team].iloc[0]\n",
    "            result = team_data['spread_result']\n",
    "            if not result:\n",
    "                parlay_win = False\n",
    "                break\n",
    "            odds = team_data['SPREAD_LINE']\n",
    "            decimal_odds = self.american_to_decimal_odds(odds)\n",
    "            parlay_odds *= decimal_odds\n",
    "        if parlay_win:\n",
    "            profit = stake * (parlay_odds - 1)\n",
    "        else:\n",
    "            profit = -stake\n",
    "        return profit\n",
    "\n",
    "    def american_to_decimal_odds(self, odds):\n",
    "        if odds > 0:\n",
    "            return (odds / 100) + 1\n",
    "        else:\n",
    "            return (100 / abs(odds)) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# Custom Policy Network Definition\n",
    "# =============================================\n",
    "\n",
    "class CustomPolicyNetwork(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space, embedding_dim=128, num_heads=4, num_layers=2):\n",
    "        self.max_games = observation_space['state_sequence'].shape[0]\n",
    "        self.output_dim = (self.max_games * embedding_dim) + embedding_dim\n",
    "        super(CustomPolicyNetwork, self).__init__(observation_space, features_dim=self.output_dim)\n",
    "        self.team_feature_dim = observation_space['state_sequence'].shape[2]\n",
    "        self.agent_feature_dim = observation_space['agent_features'].shape[0]\n",
    "        self.team_embedding = nn.Linear(self.team_feature_dim, embedding_dim)\n",
    "        self.cross_attention = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=num_heads)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.agent_embedding = nn.Linear(self.agent_feature_dim, embedding_dim)\n",
    "\n",
    "    def forward(self, observations):\n",
    "        state_sequence = observations['state_sequence']\n",
    "        agent_features = observations['agent_features']\n",
    "        if isinstance(state_sequence, np.ndarray):\n",
    "            state_sequence = torch.tensor(state_sequence, dtype=torch.float32)\n",
    "        if isinstance(agent_features, np.ndarray):\n",
    "            agent_features = torch.tensor(agent_features, dtype=torch.float32)\n",
    "        # Ensure batch dimension\n",
    "        if state_sequence.dim() == 3:\n",
    "            state_sequence = state_sequence.unsqueeze(0)\n",
    "            agent_features = agent_features.unsqueeze(0)\n",
    "        batch_size = state_sequence.shape[0]\n",
    "        max_games = state_sequence.shape[1]\n",
    "        embedding_dim = self.team_embedding.out_features\n",
    "        game_embeddings = []\n",
    "        for i in range(max_games):\n",
    "            team_a_features = state_sequence[:, i, 0, :]\n",
    "            team_b_features = state_sequence[:, i, 1, :]\n",
    "            team_a_embedding = self.team_embedding(team_a_features)\n",
    "            team_b_embedding = self.team_embedding(team_b_features)\n",
    "            team_a_embedding = team_a_embedding.unsqueeze(0)\n",
    "            team_b_embedding = team_b_embedding.unsqueeze(0)\n",
    "            attn_output_a, _ = self.cross_attention(team_a_embedding, team_b_embedding, team_b_embedding)\n",
    "            attn_output_b, _ = self.cross_attention(team_b_embedding, team_a_embedding, team_a_embedding)\n",
    "            combined_embedding = (attn_output_a + attn_output_b) / 2\n",
    "            game_embeddings.append(combined_embedding)\n",
    "        if game_embeddings:\n",
    "            game_embeddings = torch.cat(game_embeddings, dim=0)\n",
    "        else:\n",
    "            game_embeddings = torch.zeros(1, batch_size, embedding_dim).to(agent_features.device)\n",
    "        transformer_output = self.transformer_encoder(game_embeddings)\n",
    "        transformer_output = transformer_output.permute(1, 0, 2).reshape(batch_size, -1)\n",
    "        agent_embedding = self.agent_embedding(agent_features)\n",
    "        combined_input = torch.cat([transformer_output, agent_embedding], dim=1)\n",
    "        return combined_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.BettingEnv'>\n",
      "Training on device: cuda\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "<class 'sb3_contrib.common.wrappers.action_masker.ActionMasker'>\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chcro\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./ppo_betting_tensorboard/PPO_33\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 2        |\n",
      "|    ep_rew_mean     | -28.7    |\n",
      "| time/              |          |\n",
      "|    fps             | 27       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 74       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 2          |\n",
      "|    ep_rew_mean          | -72.4      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 22         |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 179        |\n",
      "|    total_timesteps      | 4096       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01130853 |\n",
      "|    clip_fraction        | 0.0983     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.68      |\n",
      "|    explained_variance   | 5.19e-06   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 1.59e+04   |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | -0.0092    |\n",
      "|    value_loss           | 3.38e+04   |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 35\u001b[0m\n\u001b[0;32m     23\u001b[0m model \u001b[38;5;241m=\u001b[39m MaskablePPO(\n\u001b[0;32m     24\u001b[0m     policy\u001b[38;5;241m=\u001b[39mMaskableMultiInputActorCriticPolicy,\n\u001b[0;32m     25\u001b[0m     env\u001b[38;5;241m=\u001b[39menv,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     31\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice  \u001b[38;5;66;03m# Specify the device here\u001b[39;00m\n\u001b[0;32m     32\u001b[0m )\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Train the agent\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m250000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Save the trained model\u001b[39;00m\n\u001b[0;32m     38\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mppo_betting_agent\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sb3_contrib\\ppo_mask\\ppo_mask.py:454\u001b[0m, in \u001b[0;36mMaskablePPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, use_masking, progress_bar)\u001b[0m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 454\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_masking\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    456\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[0;32m    457\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sb3_contrib\\ppo_mask\\ppo_mask.py:230\u001b[0m, in \u001b[0;36mMaskablePPO.collect_rollouts\u001b[1;34m(self, env, callback, rollout_buffer, n_rollout_steps, use_masking)\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_masking:\n\u001b[0;32m    228\u001b[0m         action_masks \u001b[38;5;241m=\u001b[39m get_action_masks(env)\n\u001b[1;32m--> 230\u001b[0m     actions, values, log_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_masks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction_masks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    232\u001b[0m actions \u001b[38;5;241m=\u001b[39m actions\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m    233\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(actions)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sb3_contrib\\common\\maskable\\policies.py:128\u001b[0m, in \u001b[0;36mMaskableActorCriticPolicy.forward\u001b[1;34m(self, obs, deterministic, action_masks)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;124;03mForward pass in all the networks (actor and critic)\u001b[39;00m\n\u001b[0;32m    121\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;124;03m:return: action, value and log probability of the action\u001b[39;00m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;66;03m# Preprocess the observation if needed\u001b[39;00m\n\u001b[1;32m--> 128\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshare_features_extractor:\n\u001b[0;32m    130\u001b[0m     latent_pi, latent_vf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp_extractor(features)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sb3_contrib\\common\\maskable\\policies.py:156\u001b[0m, in \u001b[0;36mMaskableActorCriticPolicy.extract_features\u001b[1;34m(self, obs, features_extractor)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;124;03mPreprocess the observation if needed and extract features.\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;124;03m    features for the actor and the features for the critic.\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshare_features_extractor:\n\u001b[1;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures_extractor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures_extractor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m features_extractor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\stable_baselines3\\common\\policies.py:131\u001b[0m, in \u001b[0;36mBaseModel.extract_features\u001b[1;34m(self, obs, features_extractor)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;124;03mPreprocess the observation if needed and extract features.\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m:return: The extracted features\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    130\u001b[0m preprocessed_obs \u001b[38;5;241m=\u001b[39m preprocess_obs(obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_space, normalize_images\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize_images)\n\u001b[1;32m--> 131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfeatures_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocessed_obs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[31], line 40\u001b[0m, in \u001b[0;36mCustomPolicyNetwork.forward\u001b[1;34m(self, observations)\u001b[0m\n\u001b[0;32m     38\u001b[0m team_a_embedding \u001b[38;5;241m=\u001b[39m team_a_embedding\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     39\u001b[0m team_b_embedding \u001b[38;5;241m=\u001b[39m team_b_embedding\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 40\u001b[0m attn_output_a, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mteam_a_embedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mteam_b_embedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mteam_b_embedding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m attn_output_b, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcross_attention(team_b_embedding, team_a_embedding, team_a_embedding)\n\u001b[0;32m     42\u001b[0m combined_embedding \u001b[38;5;241m=\u001b[39m (attn_output_a \u001b[38;5;241m+\u001b[39m attn_output_b) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\activation.py:1368\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[1;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[0;32m   1342\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[0;32m   1343\u001b[0m         query,\n\u001b[0;32m   1344\u001b[0m         key,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1365\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal,\n\u001b[0;32m   1366\u001b[0m     )\n\u001b[0;32m   1367\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1368\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1369\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1370\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1372\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1373\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1374\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1375\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1376\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1377\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1378\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1379\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1380\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1381\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1384\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1385\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1386\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1387\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1388\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[0;32m   1390\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\functional.py:6241\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[1;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[0;32m   6237\u001b[0m     attn_output_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbaddbmm(\n\u001b[0;32m   6238\u001b[0m         attn_mask, q_scaled, k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m   6239\u001b[0m     )\n\u001b[0;32m   6240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 6241\u001b[0m     attn_output_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbmm(q_scaled, \u001b[43mk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   6242\u001b[0m attn_output_weights \u001b[38;5;241m=\u001b[39m softmax(attn_output_weights, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m   6243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dropout_p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load your dataset\n",
    "data = train.groupby('DATE').filter(lambda x: (len(x) >= 4) and (len(x) <= 6))  # Replace with your data file\n",
    "\n",
    "# Create the environment\n",
    "env = BettingEnv(data=data, max_parlays=2, parlay_size=2, max_games=6)\n",
    "\n",
    "# Wrap the environment with ActionMasker\n",
    "env = ActionMasker(env, get_action_mask)\n",
    "# Determine the device: 'cuda' if available, else 'cpu'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Training on device: {device}\")\n",
    "# Define the policy_kwargs with your custom policy network\n",
    "policy_kwargs = dict(\n",
    "    features_extractor_class=CustomPolicyNetwork,\n",
    "    features_extractor_kwargs=dict(\n",
    "        embedding_dim=128,\n",
    "        num_heads=4,\n",
    "        num_layers=2,\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Instantiate the Maskable PPO agent\n",
    "model = MaskablePPO(\n",
    "    policy=MaskableMultiInputActorCriticPolicy,\n",
    "    env=env,\n",
    "    learning_rate=0.0003,\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    verbose=1,\n",
    "    tensorboard_log=\"./ppo_betting_tensorboard/\",\n",
    "    seed=SEED,\n",
    "    device=device  # Specify the device here\n",
    ")\n",
    "\n",
    "# Train the agent\n",
    "model.learn(total_timesteps=250000)\n",
    "\n",
    "# Save the trained model\n",
    "model.save('ppo_betting_agent')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device: cuda\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "<class '__main__.BettingEnv'>\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chcro\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./ppo_betting_tensorboard/PPO_35\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1        |\n",
      "|    ep_rew_mean     | -47.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 15       |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 133      |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 14.8        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 13          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 299         |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017979976 |\n",
      "|    clip_fraction        | 0.173       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -20.8       |\n",
      "|    explained_variance   | -9.48e-05   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.14e+04    |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00554    |\n",
      "|    value_loss           | 2.08e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | -38         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 13          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 456         |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018472288 |\n",
      "|    clip_fraction        | 0.241       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -20.8       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.83e+03    |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | 0.00142     |\n",
      "|    value_loss           | 2.27e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | -12.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 13          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 617         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015465077 |\n",
      "|    clip_fraction        | 0.257       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -20.7       |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.22e+04    |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00114    |\n",
      "|    value_loss           | 2.09e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | -27.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 13          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 772         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011062579 |\n",
      "|    clip_fraction        | 0.252       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -20.7       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.13e+04    |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0017     |\n",
      "|    value_loss           | 2.17e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | -25.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 13          |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 942         |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012214803 |\n",
      "|    clip_fraction        | 0.186       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -20.7       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.12e+04    |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00909    |\n",
      "|    value_loss           | 2.07e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | -44.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 12          |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 1114        |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009058917 |\n",
      "|    clip_fraction        | 0.178       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -20.7       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.4e+04     |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.000718   |\n",
      "|    value_loss           | 2.32e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | -30.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 12          |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 1275        |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011249466 |\n",
      "|    clip_fraction        | 0.137       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -20.7       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.22e+04    |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00444    |\n",
      "|    value_loss           | 2.11e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | -44.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 12          |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 1443        |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013027302 |\n",
      "|    clip_fraction        | 0.175       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -20.6       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.11e+04    |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00875    |\n",
      "|    value_loss           | 2.14e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | -15.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 12          |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 1602        |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014310527 |\n",
      "|    clip_fraction        | 0.235       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -20.6       |\n",
      "|    explained_variance   | 8.94e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.41e+04    |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | 0.00132     |\n",
      "|    value_loss           | 2.26e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | -47.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 12          |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 1767        |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011471734 |\n",
      "|    clip_fraction        | 0.253       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -20.6       |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.41e+03    |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | 0.00517     |\n",
      "|    value_loss           | 1.99e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | -36.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 12          |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 1930        |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019193467 |\n",
      "|    clip_fraction        | 0.29        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -20.6       |\n",
      "|    explained_variance   | 0.000338    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.33e+03    |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | 0.00389     |\n",
      "|    value_loss           | 2.08e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | -26.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 12          |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 2094        |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008356741 |\n",
      "|    clip_fraction        | 0.206       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -20.6       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.23e+03    |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | 0.001       |\n",
      "|    value_loss           | 2.09e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | -22.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 12          |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 2253        |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010926907 |\n",
      "|    clip_fraction        | 0.237       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -20.5       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.89e+03    |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | 0.00291     |\n",
      "|    value_loss           | 1.99e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | -19.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 12          |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 2415        |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030727142 |\n",
      "|    clip_fraction        | 0.202       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -20.5       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.18e+04    |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | 3.59e-05    |\n",
      "|    value_loss           | 2.11e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1          |\n",
      "|    ep_rew_mean          | -0.545     |\n",
      "| time/                   |            |\n",
      "|    fps                  | 12         |\n",
      "|    iterations           | 16         |\n",
      "|    time_elapsed         | 2579       |\n",
      "|    total_timesteps      | 32768      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01260921 |\n",
      "|    clip_fraction        | 0.244      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -20.4      |\n",
      "|    explained_variance   | 1.19e-07   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 8.7e+03    |\n",
      "|    n_updates            | 150        |\n",
      "|    policy_gradient_loss | 0.000896   |\n",
      "|    value_loss           | 1.96e+04   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | -29.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 12          |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 2748        |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023824904 |\n",
      "|    clip_fraction        | 0.219       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -20.4       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.38e+04    |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | 0.0023      |\n",
      "|    value_loss           | 2.12e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | -5.73       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 12          |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 2914        |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012431353 |\n",
      "|    clip_fraction        | 0.164       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -20.4       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.92e+03    |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.00256    |\n",
      "|    value_loss           | 2.1e+04     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | -20.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 12          |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 3085        |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015691467 |\n",
      "|    clip_fraction        | 0.186       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -20.3       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.88e+03    |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | 0.00129     |\n",
      "|    value_loss           | 2.06e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | -60.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 12          |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 3241        |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010826353 |\n",
      "|    clip_fraction        | 0.217       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -20.2       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.92e+03    |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.00467    |\n",
      "|    value_loss           | 2.07e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 4.36        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 12          |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 3379        |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010554314 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -20.2       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.89e+03    |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.00727    |\n",
      "|    value_loss           | 2.06e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | -49.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 12          |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 3521        |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011268992 |\n",
      "|    clip_fraction        | 0.208       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -20.1       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.04e+04    |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | 0.000367    |\n",
      "|    value_loss           | 1.89e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | -4.73       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 12          |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 3661        |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017016321 |\n",
      "|    clip_fraction        | 0.236       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -20.1       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.96e+03    |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | 0.00407     |\n",
      "|    value_loss           | 2.02e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 2.09        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 12          |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 3801        |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017591905 |\n",
      "|    clip_fraction        | 0.167       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -20.1       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.5e+03     |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.00312    |\n",
      "|    value_loss           | 1.99e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | -21.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 13          |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 3938        |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012254959 |\n",
      "|    clip_fraction        | 0.15        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -20.1       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.72e+03    |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.00307    |\n",
      "|    value_loss           | 1.98e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | -7.36       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 13          |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 4077        |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009299289 |\n",
      "|    clip_fraction        | 0.161       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -20         |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.68e+03    |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.00176    |\n",
      "|    value_loss           | 1.99e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1          |\n",
      "|    ep_rew_mean          | -18.2      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 13         |\n",
      "|    iterations           | 27         |\n",
      "|    time_elapsed         | 4218       |\n",
      "|    total_timesteps      | 55296      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01630932 |\n",
      "|    clip_fraction        | 0.194      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -20        |\n",
      "|    explained_variance   | 1.19e-07   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 8.73e+03   |\n",
      "|    n_updates            | 260        |\n",
      "|    policy_gradient_loss | 0.000944   |\n",
      "|    value_loss           | 1.93e+04   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | -20.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 13          |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 4360        |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026158676 |\n",
      "|    clip_fraction        | 0.165       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -20         |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.21e+04    |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | 0.00171     |\n",
      "|    value_loss           | 1.98e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | -14.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 13          |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 4497        |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019311018 |\n",
      "|    clip_fraction        | 0.173       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -19.9       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.98e+03    |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.00513    |\n",
      "|    value_loss           | 1.99e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | -14.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 13          |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 4639        |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011910632 |\n",
      "|    clip_fraction        | 0.254       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -19.7       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.92e+03    |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | 0.000671    |\n",
      "|    value_loss           | 2.04e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 10.4        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 13          |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 4783        |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014526367 |\n",
      "|    clip_fraction        | 0.208       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -19.7       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.14e+04    |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | 0.00162     |\n",
      "|    value_loss           | 1.93e+04    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | 0.182        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 13           |\n",
      "|    iterations           | 32           |\n",
      "|    time_elapsed         | 4922         |\n",
      "|    total_timesteps      | 65536        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0115357265 |\n",
      "|    clip_fraction        | 0.217        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -19.8        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.33e+04     |\n",
      "|    n_updates            | 310          |\n",
      "|    policy_gradient_loss | -9.01e-05    |\n",
      "|    value_loss           | 1.84e+04     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | -19.9        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 13           |\n",
      "|    iterations           | 33           |\n",
      "|    time_elapsed         | 5067         |\n",
      "|    total_timesteps      | 67584        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0151733225 |\n",
      "|    clip_fraction        | 0.27         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -19.7        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.01e+04     |\n",
      "|    n_updates            | 320          |\n",
      "|    policy_gradient_loss | -0.00263     |\n",
      "|    value_loss           | 1.97e+04     |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1          |\n",
      "|    ep_rew_mean          | -6.36      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 13         |\n",
      "|    iterations           | 34         |\n",
      "|    time_elapsed         | 5214       |\n",
      "|    total_timesteps      | 69632      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02033808 |\n",
      "|    clip_fraction        | 0.25       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -19.6      |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 1.22e+04   |\n",
      "|    n_updates            | 330        |\n",
      "|    policy_gradient_loss | 0.00246    |\n",
      "|    value_loss           | 1.85e+04   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | -15.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 13          |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 5349        |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014487325 |\n",
      "|    clip_fraction        | 0.207       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -19.4       |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.03e+03    |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.00289    |\n",
      "|    value_loss           | 1.88e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | -9.82       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 13          |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 5486        |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020205157 |\n",
      "|    clip_fraction        | 0.142       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -19.4       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.23e+03    |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.00355    |\n",
      "|    value_loss           | 1.83e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | -11.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 13          |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 5622        |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011431988 |\n",
      "|    clip_fraction        | 0.163       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -19.3       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.71e+03    |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.00203    |\n",
      "|    value_loss           | 1.81e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | -18.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 13          |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 5765        |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012574476 |\n",
      "|    clip_fraction        | 0.204       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -19.1       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.65e+03    |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.00256    |\n",
      "|    value_loss           | 1.68e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | -18.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 13          |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 5904        |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016136833 |\n",
      "|    clip_fraction        | 0.201       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -19         |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.94e+03    |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.00301    |\n",
      "|    value_loss           | 1.77e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | -13.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 13          |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 6042        |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012015926 |\n",
      "|    clip_fraction        | 0.165       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -18.9       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.18e+04    |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.00645    |\n",
      "|    value_loss           | 1.8e+04     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | -21.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 13          |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 6176        |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014378165 |\n",
      "|    clip_fraction        | 0.214       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -18.8       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.19e+03    |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.00508    |\n",
      "|    value_loss           | 1.61e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1          |\n",
      "|    ep_rew_mean          | -11.1      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 13         |\n",
      "|    iterations           | 42         |\n",
      "|    time_elapsed         | 6313       |\n",
      "|    total_timesteps      | 86016      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01301981 |\n",
      "|    clip_fraction        | 0.212      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -18.6      |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 8.48e+03   |\n",
      "|    n_updates            | 410        |\n",
      "|    policy_gradient_loss | -0.00495   |\n",
      "|    value_loss           | 1.7e+04    |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | -26.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 13          |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 6452        |\n",
      "|    total_timesteps      | 88064       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015774459 |\n",
      "|    clip_fraction        | 0.195       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -18.5       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.01e+04    |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.00285    |\n",
      "|    value_loss           | 1.58e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | -4          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 13          |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 6595        |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012463024 |\n",
      "|    clip_fraction        | 0.212       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -18.4       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.02e+03    |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.00251    |\n",
      "|    value_loss           | 1.69e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 0.364       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 13          |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 6729        |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008935478 |\n",
      "|    clip_fraction        | 0.199       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -18.4       |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.48e+04    |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.000224   |\n",
      "|    value_loss           | 1.6e+04     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 2.91        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 13          |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 6871        |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017578062 |\n",
      "|    clip_fraction        | 0.212       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -18.3       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.9e+03     |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.000429   |\n",
      "|    value_loss           | 1.53e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 4.91        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 13          |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 7003        |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012524741 |\n",
      "|    clip_fraction        | 0.145       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -18.1       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.31e+03    |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.00341    |\n",
      "|    value_loss           | 1.5e+04     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | -11         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 13          |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 7144        |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013861431 |\n",
      "|    clip_fraction        | 0.204       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -18.1       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.29e+03    |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.00089    |\n",
      "|    value_loss           | 1.44e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | -32.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 13          |\n",
      "|    iterations           | 49          |\n",
      "|    time_elapsed         | 7281        |\n",
      "|    total_timesteps      | 100352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009286613 |\n",
      "|    clip_fraction        | 0.14        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -18         |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.55e+03    |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.00547    |\n",
      "|    value_loss           | 1.55e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 11.8        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 13          |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 7421        |\n",
      "|    total_timesteps      | 102400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011780727 |\n",
      "|    clip_fraction        | 0.275       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -17.8       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.01e+04    |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.0028     |\n",
      "|    value_loss           | 1.57e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | -13.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 13          |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 7553        |\n",
      "|    total_timesteps      | 104448      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013111134 |\n",
      "|    clip_fraction        | 0.217       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -17.8       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.73e+03    |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | 0.00527     |\n",
      "|    value_loss           | 1.41e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | -6.45       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 13          |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 7685        |\n",
      "|    total_timesteps      | 106496      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013241261 |\n",
      "|    clip_fraction        | 0.257       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -17.7       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.34e+03    |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | 0.00223     |\n",
      "|    value_loss           | 1.47e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 4.36        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 13          |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 7817        |\n",
      "|    total_timesteps      | 108544      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019715898 |\n",
      "|    clip_fraction        | 0.226       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -17.8       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.15e+03    |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | 0.000132    |\n",
      "|    value_loss           | 1.5e+04     |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1          |\n",
      "|    ep_rew_mean          | -24.3      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 13         |\n",
      "|    iterations           | 54         |\n",
      "|    time_elapsed         | 7951       |\n",
      "|    total_timesteps      | 110592     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01082426 |\n",
      "|    clip_fraction        | 0.214      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -17.7      |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 8.49e+03   |\n",
      "|    n_updates            | 530        |\n",
      "|    policy_gradient_loss | -0.000692  |\n",
      "|    value_loss           | 1.48e+04   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | -16.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 13          |\n",
      "|    iterations           | 55          |\n",
      "|    time_elapsed         | 8085        |\n",
      "|    total_timesteps      | 112640      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014788936 |\n",
      "|    clip_fraction        | 0.186       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -17.7       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.29e+03    |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.00281    |\n",
      "|    value_loss           | 1.59e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | -12.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 13          |\n",
      "|    iterations           | 56          |\n",
      "|    time_elapsed         | 8220        |\n",
      "|    total_timesteps      | 114688      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011557557 |\n",
      "|    clip_fraction        | 0.191       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -17.7       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.41e+03    |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | 0.000702    |\n",
      "|    value_loss           | 1.52e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | -5.09       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 13          |\n",
      "|    iterations           | 57          |\n",
      "|    time_elapsed         | 8351        |\n",
      "|    total_timesteps      | 116736      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019714449 |\n",
      "|    clip_fraction        | 0.159       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -17.5       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.9e+03     |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.00367    |\n",
      "|    value_loss           | 1.43e+04    |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | -4.45        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 13           |\n",
      "|    iterations           | 58           |\n",
      "|    time_elapsed         | 8489         |\n",
      "|    total_timesteps      | 118784       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0094233025 |\n",
      "|    clip_fraction        | 0.123        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17.4        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.24e+03     |\n",
      "|    n_updates            | 570          |\n",
      "|    policy_gradient_loss | -0.00612     |\n",
      "|    value_loss           | 1.38e+04     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1            |\n",
      "|    ep_rew_mean          | -14.9        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 13           |\n",
      "|    iterations           | 59           |\n",
      "|    time_elapsed         | 8631         |\n",
      "|    total_timesteps      | 120832       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0113035925 |\n",
      "|    clip_fraction        | 0.259        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17.3        |\n",
      "|    explained_variance   | 0            |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.24e+03     |\n",
      "|    n_updates            | 580          |\n",
      "|    policy_gradient_loss | 0.00222      |\n",
      "|    value_loss           | 1.36e+04     |\n",
      "------------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 1        |\n",
      "|    ep_rew_mean          | -1.27    |\n",
      "| time/                   |          |\n",
      "|    fps                  | 14       |\n",
      "|    iterations           | 60       |\n",
      "|    time_elapsed         | 8768     |\n",
      "|    total_timesteps      | 122880   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.029652 |\n",
      "|    clip_fraction        | 0.223    |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -17.4    |\n",
      "|    explained_variance   | 0        |\n",
      "|    learning_rate        | 0.0003   |\n",
      "|    loss                 | 1.09e+04 |\n",
      "|    n_updates            | 590      |\n",
      "|    policy_gradient_loss | -0.00229 |\n",
      "|    value_loss           | 1.45e+04 |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1          |\n",
      "|    ep_rew_mean          | -4.55      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 14         |\n",
      "|    iterations           | 61         |\n",
      "|    time_elapsed         | 8903       |\n",
      "|    total_timesteps      | 124928     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02291781 |\n",
      "|    clip_fraction        | 0.144      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -17.3      |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 8.46e+03   |\n",
      "|    n_updates            | 600        |\n",
      "|    policy_gradient_loss | -0.00173   |\n",
      "|    value_loss           | 1.45e+04   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | -3.64       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 62          |\n",
      "|    time_elapsed         | 9037        |\n",
      "|    total_timesteps      | 126976      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010296338 |\n",
      "|    clip_fraction        | 0.193       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -17.1       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.01e+03    |\n",
      "|    n_updates            | 610         |\n",
      "|    policy_gradient_loss | -0.000336   |\n",
      "|    value_loss           | 1.34e+04    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1          |\n",
      "|    ep_rew_mean          | 12.2       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 14         |\n",
      "|    iterations           | 63         |\n",
      "|    time_elapsed         | 9178       |\n",
      "|    total_timesteps      | 129024     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02098675 |\n",
      "|    clip_fraction        | 0.201      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -17        |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 4.58e+03   |\n",
      "|    n_updates            | 620        |\n",
      "|    policy_gradient_loss | -0.00834   |\n",
      "|    value_loss           | 1.32e+04   |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1          |\n",
      "|    ep_rew_mean          | -19.9      |\n",
      "| time/                   |            |\n",
      "|    fps                  | 14         |\n",
      "|    iterations           | 64         |\n",
      "|    time_elapsed         | 9321       |\n",
      "|    total_timesteps      | 131072     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01057101 |\n",
      "|    clip_fraction        | 0.212      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -16.8      |\n",
      "|    explained_variance   | 0          |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 5.02e+03   |\n",
      "|    n_updates            | 630        |\n",
      "|    policy_gradient_loss | -0.00278   |\n",
      "|    value_loss           | 1.28e+04   |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | -10.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 65          |\n",
      "|    time_elapsed         | 9461        |\n",
      "|    total_timesteps      | 133120      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014224999 |\n",
      "|    clip_fraction        | 0.176       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -16.7       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.08e+03    |\n",
      "|    n_updates            | 640         |\n",
      "|    policy_gradient_loss | -0.00817    |\n",
      "|    value_loss           | 1.32e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 13.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 66          |\n",
      "|    time_elapsed         | 9598        |\n",
      "|    total_timesteps      | 135168      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013373498 |\n",
      "|    clip_fraction        | 0.263       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -16.7       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.73e+03    |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | 0.00535     |\n",
      "|    value_loss           | 1.39e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | -31.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 67          |\n",
      "|    time_elapsed         | 9740        |\n",
      "|    total_timesteps      | 137216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009726279 |\n",
      "|    clip_fraction        | 0.304       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -16.6       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.13e+03    |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | 0.00268     |\n",
      "|    value_loss           | 1.31e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | -9.45       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 68          |\n",
      "|    time_elapsed         | 9874        |\n",
      "|    total_timesteps      | 139264      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018857112 |\n",
      "|    clip_fraction        | 0.208       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -16.5       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.17e+03    |\n",
      "|    n_updates            | 670         |\n",
      "|    policy_gradient_loss | -0.000811   |\n",
      "|    value_loss           | 1.2e+04     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | -1.18       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 69          |\n",
      "|    time_elapsed         | 10016       |\n",
      "|    total_timesteps      | 141312      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015693113 |\n",
      "|    clip_fraction        | 0.2         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -16.4       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.81e+03    |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.00228    |\n",
      "|    value_loss           | 1.19e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | -18.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 70          |\n",
      "|    time_elapsed         | 10159       |\n",
      "|    total_timesteps      | 143360      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010398626 |\n",
      "|    clip_fraction        | 0.175       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -16.4       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.73e+03    |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | -0.00347    |\n",
      "|    value_loss           | 1.2e+04     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | -3.91       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 71          |\n",
      "|    time_elapsed         | 10299       |\n",
      "|    total_timesteps      | 145408      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011562224 |\n",
      "|    clip_fraction        | 0.175       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -16.4       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.66e+03    |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | 0.0011      |\n",
      "|    value_loss           | 1.32e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 0.636       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 72          |\n",
      "|    time_elapsed         | 10434       |\n",
      "|    total_timesteps      | 147456      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016081125 |\n",
      "|    clip_fraction        | 0.237       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -16.3       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.81e+03    |\n",
      "|    n_updates            | 710         |\n",
      "|    policy_gradient_loss | 0.00124     |\n",
      "|    value_loss           | 1.32e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | -3          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 73          |\n",
      "|    time_elapsed         | 10566       |\n",
      "|    total_timesteps      | 149504      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014607361 |\n",
      "|    clip_fraction        | 0.29        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -16.2       |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.62e+03    |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.00276    |\n",
      "|    value_loss           | 1.17e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | -13.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 74          |\n",
      "|    time_elapsed         | 10699       |\n",
      "|    total_timesteps      | 151552      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015748076 |\n",
      "|    clip_fraction        | 0.219       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -16.1       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.79e+03    |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | -0.00792    |\n",
      "|    value_loss           | 1.15e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | -21.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 75          |\n",
      "|    time_elapsed         | 10841       |\n",
      "|    total_timesteps      | 153600      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016249064 |\n",
      "|    clip_fraction        | 0.196       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -16         |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.45e+03    |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.000418   |\n",
      "|    value_loss           | 1.13e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | -2.64       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 76          |\n",
      "|    time_elapsed         | 10979       |\n",
      "|    total_timesteps      | 155648      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018619882 |\n",
      "|    clip_fraction        | 0.18        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -16         |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.57e+03    |\n",
      "|    n_updates            | 750         |\n",
      "|    policy_gradient_loss | -0.00577    |\n",
      "|    value_loss           | 1.11e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 14.1        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 77          |\n",
      "|    time_elapsed         | 11118       |\n",
      "|    total_timesteps      | 157696      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013858951 |\n",
      "|    clip_fraction        | 0.176       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -15.8       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.04e+03    |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.00698    |\n",
      "|    value_loss           | 1.09e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | -16.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 78          |\n",
      "|    time_elapsed         | 11256       |\n",
      "|    total_timesteps      | 159744      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027352672 |\n",
      "|    clip_fraction        | 0.228       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -15.8       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.26e+03    |\n",
      "|    n_updates            | 770         |\n",
      "|    policy_gradient_loss | -0.000243   |\n",
      "|    value_loss           | 1.14e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 0.909       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 79          |\n",
      "|    time_elapsed         | 11394       |\n",
      "|    total_timesteps      | 161792      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017036708 |\n",
      "|    clip_fraction        | 0.256       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -15.6       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.34e+03    |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.00422    |\n",
      "|    value_loss           | 1.07e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | -4.64       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 80          |\n",
      "|    time_elapsed         | 11531       |\n",
      "|    total_timesteps      | 163840      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010715663 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -15.5       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.27e+03    |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | -0.00415    |\n",
      "|    value_loss           | 1.08e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | -0.818      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 81          |\n",
      "|    time_elapsed         | 11673       |\n",
      "|    total_timesteps      | 165888      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014217347 |\n",
      "|    clip_fraction        | 0.26        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -15.5       |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.41e+03    |\n",
      "|    n_updates            | 800         |\n",
      "|    policy_gradient_loss | -3.97e-05   |\n",
      "|    value_loss           | 1.08e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | -7.45       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 82          |\n",
      "|    time_elapsed         | 11818       |\n",
      "|    total_timesteps      | 167936      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007671319 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -15.4       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.27e+03    |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.00502    |\n",
      "|    value_loss           | 1.1e+04     |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | -18.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 83          |\n",
      "|    time_elapsed         | 11958       |\n",
      "|    total_timesteps      | 169984      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013040915 |\n",
      "|    clip_fraction        | 0.187       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -15.3       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.12e+03    |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.00443    |\n",
      "|    value_loss           | 1.08e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | -9.55       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 84          |\n",
      "|    time_elapsed         | 12094       |\n",
      "|    total_timesteps      | 172032      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020294856 |\n",
      "|    clip_fraction        | 0.197       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -15.2       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.21e+03    |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | -0.00133    |\n",
      "|    value_loss           | 1.07e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 1.45        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 85          |\n",
      "|    time_elapsed         | 12236       |\n",
      "|    total_timesteps      | 174080      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020312786 |\n",
      "|    clip_fraction        | 0.224       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -15.2       |\n",
      "|    explained_variance   | 1.19e-07    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.18e+03    |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.003      |\n",
      "|    value_loss           | 9.79e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | -1.27       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 86          |\n",
      "|    time_elapsed         | 12383       |\n",
      "|    total_timesteps      | 176128      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012534982 |\n",
      "|    clip_fraction        | 0.205       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -15.2       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.83e+03    |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | -0.00162    |\n",
      "|    value_loss           | 1.11e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | 0.364       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 87          |\n",
      "|    time_elapsed         | 12523       |\n",
      "|    total_timesteps      | 178176      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011491738 |\n",
      "|    clip_fraction        | 0.158       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -15.1       |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.3e+03     |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.00367    |\n",
      "|    value_loss           | 1.07e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | -10.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 88          |\n",
      "|    time_elapsed         | 12661       |\n",
      "|    total_timesteps      | 180224      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009126809 |\n",
      "|    clip_fraction        | 0.17        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -15         |\n",
      "|    explained_variance   | 0           |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.4e+03     |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.00286    |\n",
      "|    value_loss           | 1.06e+04    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1           |\n",
      "|    ep_rew_mean          | -3.91       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 14          |\n",
      "|    iterations           | 89          |\n",
      "|    time_elapsed         | 12799       |\n",
      "|    total_timesteps      | 182272      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021830011 |\n",
      "|    clip_fraction        | 0.245       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -15         |\n",
      "|    explained_variance   | -1.19e-07   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.44e+03    |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.000413   |\n",
      "|    value_loss           | 1.12e+04    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 270\u001b[0m\n\u001b[0;32m    258\u001b[0m model \u001b[38;5;241m=\u001b[39m PPO(\n\u001b[0;32m    259\u001b[0m     policy\u001b[38;5;241m=\u001b[39mMultiInputActorCriticPolicy,\n\u001b[0;32m    260\u001b[0m     env\u001b[38;5;241m=\u001b[39menv,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    266\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice  \u001b[38;5;66;03m# Specify the device here\u001b[39;00m\n\u001b[0;32m    267\u001b[0m )\n\u001b[0;32m    269\u001b[0m \u001b[38;5;66;03m# Train the agent\u001b[39;00m\n\u001b[1;32m--> 270\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m250000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;66;03m# Save the trained model\u001b[39;00m\n\u001b[0;32m    273\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mppo_betting_agent\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\stable_baselines3\\ppo\\ppo.py:311\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[0;32m    304\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    309\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    310\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[1;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:323\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 323\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[0;32m    326\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:218\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    214\u001b[0m         \u001b[38;5;66;03m# Otherwise, clip the actions to avoid out of bound error\u001b[39;00m\n\u001b[0;32m    215\u001b[0m         \u001b[38;5;66;03m# as we are sampling from an unbounded Gaussian distribution\u001b[39;00m\n\u001b[0;32m    216\u001b[0m         clipped_actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(actions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mlow, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mhigh)\n\u001b[1;32m--> 218\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclipped_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[0;32m    222\u001b[0m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:206\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \n\u001b[0;32m    202\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[1;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:70\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx]:\n\u001b[0;32m     68\u001b[0m         \u001b[38;5;66;03m# save final observation where user can get it, then reset\u001b[39;00m\n\u001b[0;32m     69\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mterminal_observation\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m obs\n\u001b[1;32m---> 70\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_obs(env_idx, obs)\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obs_from_buf(), np\u001b[38;5;241m.\u001b[39mcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews), np\u001b[38;5;241m.\u001b[39mcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones), deepcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\stable_baselines3\\common\\monitor.py:83\u001b[0m, in \u001b[0;36mMonitor.reset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected you to pass keyword argument \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m into reset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_reset_info[key] \u001b[38;5;241m=\u001b[39m value\n\u001b[1;32m---> 83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[39], line 103\u001b[0m, in \u001b[0;36mBettingEnv.reset\u001b[1;34m(self, seed, options)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 103\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_sequence, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_features, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mteams \u001b[38;5;241m=\u001b[39m \u001b[43mget_state\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgames_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_games\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_teams \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mteams)\n\u001b[0;32m    105\u001b[0m \u001b[38;5;66;03m# Pad state_sequence to self.max_games\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[39], line 24\u001b[0m, in \u001b[0;36mget_state\u001b[1;34m(games_data, agent_context, max_games)\u001b[0m\n\u001b[0;32m     22\u001b[0m team_features \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m team \u001b[38;5;129;01min\u001b[39;00m teams:\n\u001b[1;32m---> 24\u001b[0m     team_data \u001b[38;5;241m=\u001b[39m \u001b[43mgame_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgame_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTEAM\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mteam\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     25\u001b[0m     features \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     26\u001b[0m         team_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspread_mean\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     27\u001b[0m         team_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSPREAD_LINE\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     35\u001b[0m         team_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspread_var\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     36\u001b[0m     ]\n\u001b[0;32m     37\u001b[0m     team_features\u001b[38;5;241m.\u001b[39mappend(features)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\frame.py:4093\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4091\u001b[0m \u001b[38;5;66;03m# Do we have a (boolean) 1d indexer?\u001b[39;00m\n\u001b[0;32m   4092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m com\u001b[38;5;241m.\u001b[39mis_bool_indexer(key):\n\u001b[1;32m-> 4093\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_bool_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4095\u001b[0m \u001b[38;5;66;03m# We are left with two options: a single key, and a collection of keys,\u001b[39;00m\n\u001b[0;32m   4096\u001b[0m \u001b[38;5;66;03m# We interpret tuples as collections only for non-MultiIndex\u001b[39;00m\n\u001b[0;32m   4097\u001b[0m is_single_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_list_like(key)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\frame.py:4155\u001b[0m, in \u001b[0;36mDataFrame._getitem_bool_array\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m   4154\u001b[0m indexer \u001b[38;5;241m=\u001b[39m key\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m-> 4155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_with_is_copy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\generic.py:4153\u001b[0m, in \u001b[0;36mNDFrame._take_with_is_copy\u001b[1;34m(self, indices, axis)\u001b[0m\n\u001b[0;32m   4142\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m   4143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_take_with_is_copy\u001b[39m(\u001b[38;5;28mself\u001b[39m, indices, axis: Axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[0;32m   4144\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4145\u001b[0m \u001b[38;5;124;03m    Internal version of the `take` method that sets the `_is_copy`\u001b[39;00m\n\u001b[0;32m   4146\u001b[0m \u001b[38;5;124;03m    attribute to keep track of the parent dataframe (using in indexing\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4151\u001b[0m \u001b[38;5;124;03m    See the docstring of `take` for full explanation of the parameters.\u001b[39;00m\n\u001b[0;32m   4152\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4153\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4154\u001b[0m     \u001b[38;5;66;03m# Maybe set copy if we didn't actually change the index.\u001b[39;00m\n\u001b[0;32m   4155\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result\u001b[38;5;241m.\u001b[39m_get_axis(axis)\u001b[38;5;241m.\u001b[39mequals(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis(axis)):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\generic.py:4133\u001b[0m, in \u001b[0;36mNDFrame.take\u001b[1;34m(self, indices, axis, **kwargs)\u001b[0m\n\u001b[0;32m   4128\u001b[0m     \u001b[38;5;66;03m# We can get here with a slice via DataFrame.__getitem__\u001b[39;00m\n\u001b[0;32m   4129\u001b[0m     indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\n\u001b[0;32m   4130\u001b[0m         indices\u001b[38;5;241m.\u001b[39mstart, indices\u001b[38;5;241m.\u001b[39mstop, indices\u001b[38;5;241m.\u001b[39mstep, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mintp\n\u001b[0;32m   4131\u001b[0m     )\n\u001b[1;32m-> 4133\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4135\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_block_manager_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   4137\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(new_data, axes\u001b[38;5;241m=\u001b[39mnew_data\u001b[38;5;241m.\u001b[39maxes)\u001b[38;5;241m.\u001b[39m__finalize__(\n\u001b[0;32m   4139\u001b[0m     \u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtake\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4140\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\internals\\managers.py:894\u001b[0m, in \u001b[0;36mBaseBlockManager.take\u001b[1;34m(self, indexer, axis, verify)\u001b[0m\n\u001b[0;32m    891\u001b[0m indexer \u001b[38;5;241m=\u001b[39m maybe_convert_indices(indexer, n, verify\u001b[38;5;241m=\u001b[39mverify)\n\u001b[0;32m    893\u001b[0m new_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes[axis]\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m--> 894\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreindex_indexer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    895\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnew_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    896\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindexer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    897\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    898\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_dups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    899\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    900\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\internals\\managers.py:688\u001b[0m, in \u001b[0;36mBaseBlockManager.reindex_indexer\u001b[1;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, only_slice, use_na_proxy)\u001b[0m\n\u001b[0;32m    680\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slice_take_blocks_ax0(\n\u001b[0;32m    681\u001b[0m         indexer,\n\u001b[0;32m    682\u001b[0m         fill_value\u001b[38;5;241m=\u001b[39mfill_value,\n\u001b[0;32m    683\u001b[0m         only_slice\u001b[38;5;241m=\u001b[39monly_slice,\n\u001b[0;32m    684\u001b[0m         use_na_proxy\u001b[38;5;241m=\u001b[39muse_na_proxy,\n\u001b[0;32m    685\u001b[0m     )\n\u001b[0;32m    686\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    687\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m--> 688\u001b[0m         \u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    689\u001b[0m \u001b[43m            \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    690\u001b[0m \u001b[43m            \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    691\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    692\u001b[0m \u001b[43m                \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mblk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfill_value\u001b[49m\n\u001b[0;32m    693\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    694\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    695\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks\n\u001b[0;32m    696\u001b[0m     ]\n\u001b[0;32m    698\u001b[0m new_axes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n\u001b[0;32m    699\u001b[0m new_axes[axis] \u001b[38;5;241m=\u001b[39m new_axis\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\internals\\blocks.py:1307\u001b[0m, in \u001b[0;36mBlock.take_nd\u001b[1;34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[0m\n\u001b[0;32m   1304\u001b[0m     allow_fill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1306\u001b[0m \u001b[38;5;66;03m# Note: algos.take_nd has upcast logic similar to coerce_to_target_dtype\u001b[39;00m\n\u001b[1;32m-> 1307\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[43malgos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_fill\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\n\u001b[0;32m   1309\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1311\u001b[0m \u001b[38;5;66;03m# Called from three places in managers, all of which satisfy\u001b[39;00m\n\u001b[0;32m   1312\u001b[0m \u001b[38;5;66;03m#  these assertions\u001b[39;00m\n\u001b[0;32m   1313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ExtensionBlock):\n\u001b[0;32m   1314\u001b[0m     \u001b[38;5;66;03m# NB: in this case, the 'axis' kwarg will be ignored in the\u001b[39;00m\n\u001b[0;32m   1315\u001b[0m     \u001b[38;5;66;03m#  algos.take_nd call above.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\array_algos\\take.py:110\u001b[0m, in \u001b[0;36mtake_nd\u001b[1;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_1d_only_ea_dtype(arr\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[0;32m    108\u001b[0m         \u001b[38;5;66;03m# i.e. DatetimeArray, TimedeltaArray\u001b[39;00m\n\u001b[0;32m    109\u001b[0m         arr \u001b[38;5;241m=\u001b[39m cast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNDArrayBackedExtensionArray\u001b[39m\u001b[38;5;124m\"\u001b[39m, arr)\n\u001b[1;32m--> 110\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[43m            \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_fill\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mtake(indexer, fill_value\u001b[38;5;241m=\u001b[39mfill_value, allow_fill\u001b[38;5;241m=\u001b[39mallow_fill)\n\u001b[0;32m    116\u001b[0m arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(arr)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\arrays\\_mixins.py:168\u001b[0m, in \u001b[0;36mNDArrayBackedExtensionArray.take\u001b[1;34m(self, indices, allow_fill, fill_value, axis)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allow_fill:\n\u001b[0;32m    166\u001b[0m     fill_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_scalar(fill_value)\n\u001b[1;32m--> 168\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ndarray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_fill\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_from_backing_data(new_data)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\algorithms.py:1234\u001b[0m, in \u001b[0;36mtake\u001b[1;34m(arr, indices, axis, allow_fill, fill_value)\u001b[0m\n\u001b[0;32m   1231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allow_fill:\n\u001b[0;32m   1232\u001b[0m     \u001b[38;5;66;03m# Pandas style, -1 means NA\u001b[39;00m\n\u001b[0;32m   1233\u001b[0m     validate_indices(indices, arr\u001b[38;5;241m.\u001b[39mshape[axis])\n\u001b[1;32m-> 1234\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1235\u001b[0m \u001b[43m        \u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\n\u001b[0;32m   1236\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1237\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1238\u001b[0m     \u001b[38;5;66;03m# NumPy style\u001b[39;00m\n\u001b[0;32m   1239\u001b[0m     result \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mtake(indices, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\array_algos\\take.py:117\u001b[0m, in \u001b[0;36mtake_nd\u001b[1;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mtake(indexer, fill_value\u001b[38;5;241m=\u001b[39mfill_value, allow_fill\u001b[38;5;241m=\u001b[39mallow_fill)\n\u001b[0;32m    116\u001b[0m arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(arr)\n\u001b[1;32m--> 117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_take_nd_ndarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\array_algos\\take.py:162\u001b[0m, in \u001b[0;36m_take_nd_ndarray\u001b[1;34m(arr, indexer, axis, fill_value, allow_fill)\u001b[0m\n\u001b[0;32m    157\u001b[0m     out \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(out_shape, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    159\u001b[0m func \u001b[38;5;241m=\u001b[39m _get_take_nd_function(\n\u001b[0;32m    160\u001b[0m     arr\u001b[38;5;241m.\u001b[39mndim, arr\u001b[38;5;241m.\u001b[39mdtype, out\u001b[38;5;241m.\u001b[39mdtype, axis\u001b[38;5;241m=\u001b[39maxis, mask_info\u001b[38;5;241m=\u001b[39mmask_info\n\u001b[0;32m    161\u001b[0m )\n\u001b[1;32m--> 162\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m flip_order:\n\u001b[0;32m    165\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mT\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\pandas\\core\\array_algos\\take.py:369\u001b[0m, in \u001b[0;36m_view_wrapper.<locals>.wrapper\u001b[1;34m(arr, indexer, out, fill_value)\u001b[0m\n\u001b[0;32m    366\u001b[0m         fill_value \u001b[38;5;241m=\u001b[39m fill_value\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mM8[ns]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    367\u001b[0m     fill_value \u001b[38;5;241m=\u001b[39m fill_wrap(fill_value)\n\u001b[1;32m--> 369\u001b[0m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# Data Preparation Functions\n",
    "# =============================================\n",
    "\n",
    "def odds_to_implied_prob(odds):\n",
    "    if odds > 0:\n",
    "        return 100 / (odds + 100)\n",
    "    else:\n",
    "        return abs(odds) / (abs(odds) + 100)\n",
    "\n",
    "def get_state(games_data, agent_context, max_games=15):\n",
    "    state_sequence = []\n",
    "    teams_list = []\n",
    "    game_ids = games_data['GAME-ID'].unique()\n",
    "    for idx, game_id in enumerate(game_ids):\n",
    "        if idx >= max_games:\n",
    "            break\n",
    "        game_df = games_data[games_data['GAME-ID'] == game_id]\n",
    "        teams = game_df['TEAM'].unique()\n",
    "        if len(teams) < 2:\n",
    "            continue\n",
    "        team_features = []\n",
    "        for team in teams:\n",
    "            team_data = game_df[game_df['TEAM'] == team].iloc[0]\n",
    "            features = [\n",
    "                team_data['spread_mean'],\n",
    "                team_data['SPREAD_LINE'],\n",
    "                team_data['spread_value'],\n",
    "                # team_data['spread_prob1'],\n",
    "                # team_data['spread_prob2'],\n",
    "                # team_data['spread_prob3'],\n",
    "                # team_data['spread_prob4'],\n",
    "                # team_data['spread_prob5'],\n",
    "                team_data['implied_spread_odds'],\n",
    "                team_data['spread_var']\n",
    "            ]\n",
    "            team_features.append(features)\n",
    "            teams_list.append(team)\n",
    "        if len(team_features) == 2:\n",
    "            state_sequence.append(team_features)\n",
    "    agent_features = np.array([\n",
    "        agent_context['bankroll'] / agent_context['initial_bankroll'],\n",
    "    ])\n",
    "    return state_sequence, agent_features, teams_list\n",
    "\n",
    "# =============================================\n",
    "# Custom Environment Definition\n",
    "# =============================================\n",
    "\n",
    "class BettingEnv(gym.Env):\n",
    "    metadata = {'render_modes': ['human']}\n",
    "\n",
    "    def __init__(self, data, max_games=15):\n",
    "        super(BettingEnv, self).__init__()\n",
    "        self.data = data\n",
    "        self.dates = self.data['DATE'].unique()\n",
    "        self.max_games = max_games\n",
    "        self.max_teams = self.max_games * 2\n",
    "\n",
    "        # Define the action space\n",
    "        self.action_space = spaces.MultiBinary(self.max_teams)\n",
    "        self.observation_space = spaces.Dict({\n",
    "            'state_sequence': spaces.Box(\n",
    "                low=-np.inf,\n",
    "                high=np.inf,\n",
    "                shape=(self.max_games, 2, 5),\n",
    "                dtype=np.float32\n",
    "            ),\n",
    "            'agent_features': spaces.Box(\n",
    "                low=-np.inf,\n",
    "                high=np.inf,\n",
    "                shape=(1,),\n",
    "                dtype=np.float32\n",
    "            )\n",
    "        })\n",
    "        self.current_date_idx = -1\n",
    "        self.current_date = None\n",
    "        self.games_data = None\n",
    "        self.agent_context = None\n",
    "        self.state_sequence = None\n",
    "        self.agent_features = None\n",
    "        self.teams = None\n",
    "        self.num_teams = None\n",
    "        self.total_stake = 0\n",
    "        self.step_count = 0\n",
    "        self.done = False\n",
    "        self.seed(SEED)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        if seed is not None:\n",
    "            self.seed(seed)\n",
    "        self.current_date_idx = (self.current_date_idx + 1) % len(self.dates)\n",
    "        self.current_date = self.dates[self.current_date_idx]\n",
    "        self.games_data = self.data[self.data['DATE'] == self.current_date].reset_index(drop=True)\n",
    "        self.agent_context = {\n",
    "            'bankroll': 10000,\n",
    "            'initial_bankroll': 10000\n",
    "        }\n",
    "        self.total_stake = 0\n",
    "        self.step_count = 0\n",
    "        self.done = False\n",
    "        self.state_sequence, self.agent_features, self.teams = get_state(self.games_data, self.agent_context, self.max_games)\n",
    "        self.num_teams = len(self.teams)\n",
    "        # Pad state_sequence to self.max_games\n",
    "        num_games = len(self.state_sequence)\n",
    "        if num_games < self.max_games:\n",
    "            padding = [[[0]*5, [0]*5] for _ in range(self.max_games - num_games)]\n",
    "            self.state_sequence.extend(padding)\n",
    "        self.state_sequence = np.array(self.state_sequence, dtype=np.float32)\n",
    "        # Pad self.teams to self.max_teams\n",
    "        if self.num_teams < self.max_teams:\n",
    "            padding = [None] * (self.max_teams - self.num_teams)\n",
    "            self.teams.extend(padding)\n",
    "        observation = self._get_observation()\n",
    "        return observation, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        self.step_count += 1\n",
    "        reward = 0\n",
    "        stake_per_bet = 100\n",
    "        for idx in range(self.num_teams):\n",
    "            team = self.teams[idx]\n",
    "            if team is None:\n",
    "                continue  # Skip padding teams\n",
    "            if action[idx] == 1:\n",
    "                team_data = self.games_data[self.games_data['TEAM'] == team].iloc[0]\n",
    "                odds = team_data['SPREAD_LINE']\n",
    "                decimal_odds = self.american_to_decimal_odds(odds)\n",
    "                result = team_data['spread_result']\n",
    "                self.agent_context['bankroll'] -= stake_per_bet  # Deduct the stake\n",
    "                if result:\n",
    "                    profit = stake_per_bet * (decimal_odds - 1)\n",
    "                else:\n",
    "                    profit = -stake_per_bet\n",
    "                reward += profit\n",
    "                self.agent_context['bankroll'] += stake_per_bet + profit  # Update bankroll\n",
    "        self.agent_features = np.array([\n",
    "            self.agent_context['bankroll'] / self.agent_context['initial_bankroll'],\n",
    "        ])\n",
    "        self.done = True  # Episode ends after one step (one day's slate)\n",
    "        observation = self._get_observation()\n",
    "        terminated = self.done\n",
    "        truncated = False\n",
    "        return observation, reward, terminated, truncated, {}\n",
    "\n",
    "    def _get_observation(self):\n",
    "        observation = {\n",
    "            'state_sequence': self.state_sequence,\n",
    "            'agent_features': self.agent_features.astype(np.float32)\n",
    "        }\n",
    "        return observation\n",
    "\n",
    "    def render(self):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        set_random_seed(seed)\n",
    "\n",
    "    def american_to_decimal_odds(self, odds):\n",
    "        if odds > 0:\n",
    "            return (odds / 100) + 1\n",
    "        else:\n",
    "            return (100 / abs(odds)) + 1\n",
    "\n",
    "# =============================================\n",
    "# Custom Policy Network Definition\n",
    "# =============================================\n",
    "\n",
    "class CustomPolicyNetwork(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space, embedding_dim=128, num_heads=4, num_layers=2):\n",
    "        self.max_games = observation_space['state_sequence'].shape[0]\n",
    "        self.max_teams = self.max_games * 2\n",
    "        self.output_dim = (self.max_teams * embedding_dim) + embedding_dim\n",
    "        super(CustomPolicyNetwork, self).__init__(observation_space, features_dim=self.output_dim)\n",
    "        self.team_feature_dim = observation_space['state_sequence'].shape[2]\n",
    "        self.agent_feature_dim = observation_space['agent_features'].shape[0]\n",
    "        self.team_embedding = nn.Linear(self.team_feature_dim, embedding_dim)\n",
    "        self.cross_attention = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=num_heads)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.agent_embedding = nn.Linear(self.agent_feature_dim, embedding_dim)\n",
    "\n",
    "    def forward(self, observations):\n",
    "        state_sequence = observations['state_sequence']  # Shape: (batch_size, max_games, 2, num_features)\n",
    "        agent_features = observations['agent_features']  # Shape: (batch_size, agent_feature_dim)\n",
    "        if isinstance(state_sequence, np.ndarray):\n",
    "            state_sequence = torch.tensor(state_sequence, dtype=torch.float32)\n",
    "        if isinstance(agent_features, np.ndarray):\n",
    "            agent_features = torch.tensor(agent_features, dtype=torch.float32)\n",
    "        # Ensure batch dimension\n",
    "        if state_sequence.dim() == 3:\n",
    "            state_sequence = state_sequence.unsqueeze(0)\n",
    "            agent_features = agent_features.unsqueeze(0)\n",
    "        batch_size = state_sequence.shape[0]\n",
    "        per_team_embeddings = []\n",
    "        num_games = state_sequence.shape[1]\n",
    "        for i in range(num_games):\n",
    "            team_a_features = state_sequence[:, i, 0, :]  # Shape: (batch_size, num_features)\n",
    "            team_b_features = state_sequence[:, i, 1, :]  # Shape: (batch_size, num_features)\n",
    "            if torch.all(team_a_features == 0) and torch.all(team_b_features == 0):\n",
    "                continue  # Skip padding games\n",
    "            team_a_embedding = self.team_embedding(team_a_features)  # Shape: (batch_size, embedding_dim)\n",
    "            team_b_embedding = self.team_embedding(team_b_features)\n",
    "            # Reshape for multihead attention\n",
    "            team_a_embedding = team_a_embedding.unsqueeze(0)  # Shape: (1, batch_size, embedding_dim)\n",
    "            team_b_embedding = team_b_embedding.unsqueeze(0)\n",
    "            # Cross attention\n",
    "            attn_output_a, _ = self.cross_attention(team_a_embedding, team_b_embedding, team_b_embedding)\n",
    "            attn_output_b, _ = self.cross_attention(team_b_embedding, team_a_embedding, team_a_embedding)\n",
    "            per_team_embeddings.append(attn_output_a.squeeze(0))  # Shape: (batch_size, embedding_dim)\n",
    "            per_team_embeddings.append(attn_output_b.squeeze(0))\n",
    "        # Stack per_team_embeddings\n",
    "        if per_team_embeddings:\n",
    "            per_team_embeddings = torch.stack(per_team_embeddings, dim=1)  # Shape: (batch_size, num_teams, embedding_dim)\n",
    "        else:\n",
    "            # Handle case when no games\n",
    "            per_team_embeddings = torch.zeros(batch_size, 0, self.team_embedding.out_features).to(agent_features.device)\n",
    "        # Pad per_team_embeddings to self.max_teams\n",
    "        num_teams = per_team_embeddings.shape[1]\n",
    "        if num_teams < self.max_teams:\n",
    "            padding = torch.zeros(batch_size, self.max_teams - num_teams, self.team_embedding.out_features).to(agent_features.device)\n",
    "            per_team_embeddings = torch.cat([per_team_embeddings, padding], dim=1)\n",
    "        # Transformer expects input of shape (seq_len, batch_size, embedding_dim)\n",
    "        transformer_input = per_team_embeddings.permute(1, 0, 2)  # Shape: (self.max_teams, batch_size, embedding_dim)\n",
    "        transformer_output = self.transformer_encoder(transformer_input)\n",
    "        transformer_output = transformer_output.permute(1, 0, 2).reshape(batch_size, -1)  # Shape: (batch_size, self.max_teams * embedding_dim)\n",
    "        agent_embedding = self.agent_embedding(agent_features)\n",
    "        combined_input = torch.cat([transformer_output, agent_embedding], dim=1)\n",
    "        return combined_input\n",
    "\n",
    "# Load your dataset\n",
    "data = train.groupby('DATE').filter(lambda x: (len(x) >= 4) and (len(x) <= 15))  # Adjusted to 15 games\n",
    "\n",
    "# Create the environment\n",
    "env = BettingEnv(data=data, max_games=15)\n",
    "\n",
    "# Determine the device: 'cuda' if available, else 'cpu'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Training on device: {device}\")\n",
    "\n",
    "# Define the policy_kwargs with your custom policy network\n",
    "policy_kwargs = dict(\n",
    "    features_extractor_class=CustomPolicyNetwork,\n",
    "    features_extractor_kwargs=dict(\n",
    "        embedding_dim=128,\n",
    "        num_heads=4,\n",
    "        num_layers=2,\n",
    "    ),\n",
    ")\n",
    "\n",
    "from stable_baselines3.common.policies import MultiInputActorCriticPolicy\n",
    "\n",
    "# Instantiate the PPO agent\n",
    "model = PPO(\n",
    "    policy=MultiInputActorCriticPolicy,\n",
    "    env=env,\n",
    "    learning_rate=0.0003,\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    verbose=1,\n",
    "    tensorboard_log=\"./ppo_betting_tensorboard/\",\n",
    "    seed=SEED,\n",
    "    device=device  # Specify the device here\n",
    ")\n",
    "\n",
    "# Train the agent\n",
    "model.learn(total_timesteps=250000)\n",
    "\n",
    "# Save the trained model\n",
    "model.save('ppo_betting_agent')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================\n",
    "# Evaluating the Trained Agent\n",
    "# =============================================\n",
    "\n",
    "obs, _ = env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "while not done:\n",
    "    action_masks = get_action_mask(env)\n",
    "    action, _states = model.predict(obs, action_masks=action_masks)\n",
    "    obs, reward, terminated, truncated, _ = env.step(action)\n",
    "    total_reward += reward\n",
    "    done = terminated or truncated\n",
    "print(\"Total reward from the episode:\", total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device: cuda\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "<class '__main__.BettingEnv'>\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'CustomActorCriticPolicy' object has no attribute 'mlp_extractor'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 282\u001b[0m\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# No need to build additional networks\u001b[39;00m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;66;03m# Instantiate the PPO agent\u001b[39;00m\n\u001b[1;32m--> 282\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mPPO\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCustomActorCriticPolicy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0003\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpolicy_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicy_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    288\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensorboard_log\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./ppo_betting_tensorboard/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    289\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSEED\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Specify the device here\u001b[39;49;00m\n\u001b[0;32m    291\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;66;03m# Train the agent\u001b[39;00m\n\u001b[0;32m    294\u001b[0m model\u001b[38;5;241m.\u001b[39mlearn(total_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m250000\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\stable_baselines3\\ppo\\ppo.py:171\u001b[0m, in \u001b[0;36mPPO.__init__\u001b[1;34m(self, policy, env, learning_rate, n_steps, batch_size, n_epochs, gamma, gae_lambda, clip_range, clip_range_vf, normalize_advantage, ent_coef, vf_coef, max_grad_norm, use_sde, sde_sample_freq, rollout_buffer_class, rollout_buffer_kwargs, target_kl, stats_window_size, tensorboard_log, policy_kwargs, verbose, seed, device, _init_setup_model)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_kl \u001b[38;5;241m=\u001b[39m target_kl\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _init_setup_model:\n\u001b[1;32m--> 171\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\stable_baselines3\\ppo\\ppo.py:174\u001b[0m, in \u001b[0;36mPPO._setup_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_setup_model\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 174\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;66;03m# Initialize schedules for policy/value clipping\u001b[39;00m\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclip_range \u001b[38;5;241m=\u001b[39m get_schedule_fn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclip_range)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:135\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm._setup_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrollout_buffer_class \u001b[38;5;241m=\u001b[39m RolloutBuffer\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrollout_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrollout_buffer_class(\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_steps,\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation_space,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrollout_buffer_kwargs,\n\u001b[0;32m    134\u001b[0m )\n\u001b[1;32m--> 135\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[assignment]\u001b[39;49;00m\n\u001b[0;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlr_schedule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_sde\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_sde\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_kwargs\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;66;03m# Warn when not using CPU with MlpPolicy\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[40], line 275\u001b[0m, in \u001b[0;36mCustomActorCriticPolicy.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 275\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mCustomActorCriticPolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\stable_baselines3\\common\\policies.py:891\u001b[0m, in \u001b[0;36mMultiInputActorCriticPolicy.__init__\u001b[1;34m(self, observation_space, action_space, lr_schedule, net_arch, activation_fn, ortho_init, use_sde, log_std_init, full_std, use_expln, squash_output, features_extractor_class, features_extractor_kwargs, share_features_extractor, normalize_images, optimizer_class, optimizer_kwargs)\u001b[0m\n\u001b[0;32m    871\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    872\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    873\u001b[0m     observation_space: spaces\u001b[38;5;241m.\u001b[39mDict,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    889\u001b[0m     optimizer_kwargs: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Any]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    890\u001b[0m ):\n\u001b[1;32m--> 891\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobservation_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    893\u001b[0m \u001b[43m        \u001b[49m\u001b[43maction_space\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    894\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr_schedule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    895\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnet_arch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    896\u001b[0m \u001b[43m        \u001b[49m\u001b[43mactivation_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    897\u001b[0m \u001b[43m        \u001b[49m\u001b[43mortho_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    898\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_sde\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    899\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_std_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    900\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfull_std\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    901\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_expln\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[43m        \u001b[49m\u001b[43msquash_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    903\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeatures_extractor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    904\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeatures_extractor_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    905\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshare_features_extractor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    906\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnormalize_images\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    907\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    908\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    909\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\stable_baselines3\\common\\policies.py:535\u001b[0m, in \u001b[0;36mActorCriticPolicy.__init__\u001b[1;34m(self, observation_space, action_space, lr_schedule, net_arch, activation_fn, ortho_init, use_sde, log_std_init, full_std, use_expln, squash_output, features_extractor_class, features_extractor_kwargs, share_features_extractor, normalize_images, optimizer_class, optimizer_kwargs)\u001b[0m\n\u001b[0;32m    532\u001b[0m \u001b[38;5;66;03m# Action distribution\u001b[39;00m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_dist \u001b[38;5;241m=\u001b[39m make_proba_distribution(action_space, use_sde\u001b[38;5;241m=\u001b[39muse_sde, dist_kwargs\u001b[38;5;241m=\u001b[39mdist_kwargs)\n\u001b[1;32m--> 535\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlr_schedule\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\stable_baselines3\\common\\policies.py:594\u001b[0m, in \u001b[0;36mActorCriticPolicy._build\u001b[1;34m(self, lr_schedule)\u001b[0m\n\u001b[0;32m    586\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    587\u001b[0m \u001b[38;5;124;03mCreate the networks and the optimizer.\u001b[39;00m\n\u001b[0;32m    588\u001b[0m \n\u001b[0;32m    589\u001b[0m \u001b[38;5;124;03m:param lr_schedule: Learning rate schedule\u001b[39;00m\n\u001b[0;32m    590\u001b[0m \u001b[38;5;124;03m    lr_schedule(1) is the initial learning rate\u001b[39;00m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    592\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_mlp_extractor()\n\u001b[1;32m--> 594\u001b[0m latent_dim_pi \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp_extractor\u001b[49m\u001b[38;5;241m.\u001b[39mlatent_dim_pi\n\u001b[0;32m    596\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_dist, DiagGaussianDistribution):\n\u001b[0;32m    597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_net, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_std \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_dist\u001b[38;5;241m.\u001b[39mproba_distribution_net(\n\u001b[0;32m    598\u001b[0m         latent_dim\u001b[38;5;241m=\u001b[39mlatent_dim_pi, log_std_init\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_std_init\n\u001b[0;32m    599\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1931\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1929\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1930\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1931\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m   1932\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1933\u001b[0m )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'CustomActorCriticPolicy' object has no attribute 'mlp_extractor'"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# Data Preparation Functions\n",
    "# =============================================\n",
    "\n",
    "def odds_to_implied_prob(odds):\n",
    "    if odds > 0:\n",
    "        return 100 / (odds + 100)\n",
    "    else:\n",
    "        return abs(odds) / (abs(odds) + 100)\n",
    "\n",
    "def get_state(games_data, agent_context, max_games=15):\n",
    "    state_sequence = []\n",
    "    teams_list = []\n",
    "    game_ids = games_data['GAME-ID'].unique()\n",
    "    for idx, game_id in enumerate(game_ids):\n",
    "        if idx >= max_games:\n",
    "            break\n",
    "        game_df = games_data[games_data['GAME-ID'] == game_id]\n",
    "        teams = game_df['TEAM'].unique()\n",
    "        if len(teams) < 2:\n",
    "            continue\n",
    "        team_features = []\n",
    "        for team in teams:\n",
    "            team_data = game_df[game_df['TEAM'] == team].iloc[0]\n",
    "            features = [\n",
    "                team_data['spread_mean'],\n",
    "                team_data['SPREAD_LINE'],\n",
    "                team_data['spread_value'],\n",
    "                team_data['spread_prob1'],\n",
    "                team_data['spread_prob2'],\n",
    "                team_data['spread_prob3'],\n",
    "                team_data['spread_prob4'],\n",
    "                team_data['spread_prob5'],\n",
    "                team_data['implied_spread_odds'],\n",
    "                team_data['spread_var']\n",
    "            ]\n",
    "            team_features.append(features)\n",
    "            teams_list.append(team)\n",
    "        if len(team_features) == 2:\n",
    "            state_sequence.append(team_features)\n",
    "    agent_features = np.array([\n",
    "        agent_context['bankroll'] / agent_context['initial_bankroll'],\n",
    "    ])\n",
    "    return state_sequence, agent_features, teams_list\n",
    "\n",
    "# =============================================\n",
    "# Custom Environment Definition\n",
    "# =============================================\n",
    "\n",
    "class BettingEnv(gym.Env):\n",
    "    metadata = {'render_modes': ['human']}\n",
    "\n",
    "    def __init__(self, data, max_games=15, K=0.05):\n",
    "        super(BettingEnv, self).__init__()\n",
    "        self.data = data\n",
    "        self.dates = self.data['DATE'].unique()\n",
    "        self.max_games = max_games\n",
    "        self.max_teams = self.max_games * 2\n",
    "        self.K = K  # Threshold for minimum bet size\n",
    "\n",
    "        # Define the action space: continuous values between 0 and 1\n",
    "        self.action_space = spaces.Box(low=0.0, high=1.0, shape=(self.max_teams,), dtype=np.float32)\n",
    "        self.observation_space = spaces.Dict({\n",
    "            'state_sequence': spaces.Box(\n",
    "                low=-np.inf,\n",
    "                high=np.inf,\n",
    "                shape=(self.max_games, 2, 10),\n",
    "                dtype=np.float32\n",
    "            ),\n",
    "            'agent_features': spaces.Box(\n",
    "                low=-np.inf,\n",
    "                high=np.inf,\n",
    "                shape=(1,),\n",
    "                dtype=np.float32\n",
    "            )\n",
    "        })\n",
    "        self.current_date_idx = -1\n",
    "        self.current_date = None\n",
    "        self.games_data = None\n",
    "        self.agent_context = None\n",
    "        self.state_sequence = None\n",
    "        self.agent_features = None\n",
    "        self.teams = None\n",
    "        self.num_teams = None\n",
    "        self.total_stake = 0\n",
    "        self.step_count = 0\n",
    "        self.done = False\n",
    "        self.seed(SEED)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        if seed is not None:\n",
    "            self.seed(seed)\n",
    "        self.current_date_idx = (self.current_date_idx + 1) % len(self.dates)\n",
    "        self.current_date = self.dates[self.current_date_idx]\n",
    "        self.games_data = self.data[self.data['DATE'] == self.current_date].reset_index(drop=True)\n",
    "        self.agent_context = {\n",
    "            'bankroll': 10000,\n",
    "            'initial_bankroll': 10000\n",
    "        }\n",
    "        self.total_stake = 0\n",
    "        self.step_count = 0\n",
    "        self.done = False\n",
    "        self.state_sequence, self.agent_features, self.teams = get_state(self.games_data, self.agent_context, self.max_games)\n",
    "        self.num_teams = len(self.teams)\n",
    "        # Pad state_sequence to self.max_games\n",
    "        num_games = len(self.state_sequence)\n",
    "        if num_games < self.max_games:\n",
    "            padding = [[[0]*10, [0]*10] for _ in range(self.max_games - num_games)]\n",
    "            self.state_sequence.extend(padding)\n",
    "        self.state_sequence = np.array(self.state_sequence, dtype=np.float32)\n",
    "        # Pad self.teams to self.max_teams\n",
    "        if self.num_teams < self.max_teams:\n",
    "            padding = [None] * (self.max_teams - self.num_teams)\n",
    "            self.teams.extend(padding)\n",
    "        observation = self._get_observation()\n",
    "        return observation, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        self.step_count += 1\n",
    "        reward = 0\n",
    "        unit_stake = 100  # Define a unit stake\n",
    "        for idx in range(self.num_teams):\n",
    "            team = self.teams[idx]\n",
    "            if team is None:\n",
    "                continue  # Skip padding teams\n",
    "            action_value = action[idx]\n",
    "            if action_value < self.K:\n",
    "                continue  # Bet size is zero, skip\n",
    "            bet_size = action_value * unit_stake  # Calculate bet size\n",
    "            team_data = self.games_data[self.games_data['TEAM'] == team].iloc[0]\n",
    "            odds = team_data['SPREAD_LINE']\n",
    "            decimal_odds = self.american_to_decimal_odds(odds)\n",
    "            result = team_data['spread_result']\n",
    "            self.agent_context['bankroll'] -= bet_size  # Deduct the stake\n",
    "            if result:\n",
    "                profit = bet_size * (decimal_odds - 1)\n",
    "            else:\n",
    "                profit = -bet_size\n",
    "            reward += profit\n",
    "            self.agent_context['bankroll'] += bet_size + profit  # Update bankroll\n",
    "        self.agent_features = np.array([\n",
    "            self.agent_context['bankroll'] / self.agent_context['initial_bankroll'],\n",
    "        ])\n",
    "        self.done = True  # Episode ends after one step (one day's slate)\n",
    "        observation = self._get_observation()\n",
    "        terminated = self.done\n",
    "        truncated = False\n",
    "        return observation, reward, terminated, truncated, {}\n",
    "\n",
    "    def _get_observation(self):\n",
    "        observation = {\n",
    "            'state_sequence': self.state_sequence,\n",
    "            'agent_features': self.agent_features.astype(np.float32)\n",
    "        }\n",
    "        return observation\n",
    "\n",
    "    def render(self):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        set_random_seed(seed)\n",
    "\n",
    "    def american_to_decimal_odds(self, odds):\n",
    "        if odds > 0:\n",
    "            return (odds / 100) + 1\n",
    "        else:\n",
    "            return (100 / abs(odds)) + 1\n",
    "\n",
    "# =============================================\n",
    "# Custom Policy Network Definition\n",
    "# =============================================\n",
    "\n",
    "class CustomPolicyNetwork(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space, embedding_dim=128, num_heads=4, num_layers=2):\n",
    "        self.max_games = observation_space['state_sequence'].shape[0]\n",
    "        self.max_teams = self.max_games * 2\n",
    "        super(CustomPolicyNetwork, self).__init__(observation_space, features_dim=1)  # features_dim will be updated later\n",
    "        self.team_feature_dim = observation_space['state_sequence'].shape[2]\n",
    "        self.agent_feature_dim = observation_space['agent_features'].shape[0]\n",
    "        self.team_embedding = nn.Linear(self.team_feature_dim, embedding_dim)\n",
    "        self.cross_attention = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=num_heads)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.agent_embedding = nn.Linear(self.agent_feature_dim, embedding_dim)\n",
    "        # Output layer for action probabilities\n",
    "        self.output_layer = nn.Linear(self.max_teams * embedding_dim + embedding_dim, self.max_teams)\n",
    "        # Update features_dim\n",
    "        self._features_dim = self.max_teams\n",
    "\n",
    "    def forward(self, observations):\n",
    "        state_sequence = observations['state_sequence']  # Shape: (batch_size, max_games, 2, num_features)\n",
    "        agent_features = observations['agent_features']  # Shape: (batch_size, agent_feature_dim)\n",
    "        if isinstance(state_sequence, np.ndarray):\n",
    "            state_sequence = torch.tensor(state_sequence, dtype=torch.float32)\n",
    "        if isinstance(agent_features, np.ndarray):\n",
    "            agent_features = torch.tensor(agent_features, dtype=torch.float32)\n",
    "        # Ensure batch dimension\n",
    "        if state_sequence.dim() == 3:\n",
    "            state_sequence = state_sequence.unsqueeze(0)\n",
    "            agent_features = agent_features.unsqueeze(0)\n",
    "        batch_size = state_sequence.shape[0]\n",
    "        per_team_embeddings = []\n",
    "        num_games = state_sequence.shape[1]\n",
    "        for i in range(num_games):\n",
    "            team_a_features = state_sequence[:, i, 0, :]  # Shape: (batch_size, num_features)\n",
    "            team_b_features = state_sequence[:, i, 1, :]  # Shape: (batch_size, num_features)\n",
    "            if torch.all(team_a_features == 0) and torch.all(team_b_features == 0):\n",
    "                continue  # Skip padding games\n",
    "            team_a_embedding = self.team_embedding(team_a_features)  # Shape: (batch_size, embedding_dim)\n",
    "            team_b_embedding = self.team_embedding(team_b_features)\n",
    "            # Reshape for multihead attention\n",
    "            team_a_embedding = team_a_embedding.unsqueeze(0)  # Shape: (1, batch_size, embedding_dim)\n",
    "            team_b_embedding = team_b_embedding.unsqueeze(0)\n",
    "            # Cross attention\n",
    "            attn_output_a, _ = self.cross_attention(team_a_embedding, team_b_embedding, team_b_embedding)\n",
    "            attn_output_b, _ = self.cross_attention(team_b_embedding, team_a_embedding, team_a_embedding)\n",
    "            per_team_embeddings.append(attn_output_a.squeeze(0))  # Shape: (batch_size, embedding_dim)\n",
    "            per_team_embeddings.append(attn_output_b.squeeze(0))\n",
    "        # Stack per_team_embeddings\n",
    "        if per_team_embeddings:\n",
    "            per_team_embeddings = torch.stack(per_team_embeddings, dim=1)  # Shape: (batch_size, num_teams, embedding_dim)\n",
    "        else:\n",
    "            # Handle case when no games\n",
    "            per_team_embeddings = torch.zeros(batch_size, 0, self.team_embedding.out_features).to(agent_features.device)\n",
    "        # Pad per_team_embeddings to self.max_teams\n",
    "        num_teams = per_team_embeddings.shape[1]\n",
    "        if num_teams < self.max_teams:\n",
    "            padding = torch.zeros(batch_size, self.max_teams - num_teams, self.team_embedding.out_features).to(agent_features.device)\n",
    "            per_team_embeddings = torch.cat([per_team_embeddings, padding], dim=1)\n",
    "        # Transformer expects input of shape (seq_len, batch_size, embedding_dim)\n",
    "        transformer_input = per_team_embeddings.permute(1, 0, 2)  # Shape: (self.max_teams, batch_size, embedding_dim)\n",
    "        transformer_output = self.transformer_encoder(transformer_input)\n",
    "        transformer_output = transformer_output.permute(1, 0, 2).reshape(batch_size, -1)  # Shape: (batch_size, self.max_teams * embedding_dim)\n",
    "        agent_embedding = self.agent_embedding(agent_features)\n",
    "        combined_input = torch.cat([transformer_output, agent_embedding], dim=1)  # Shape: (batch_size, total_embedding_dim)\n",
    "        # Output layer to get action values\n",
    "        action_values = self.output_layer(combined_input)  # Shape: (batch_size, self.max_teams)\n",
    "        # Apply sigmoid to get values between 0 and 1\n",
    "        action_probs = torch.sigmoid(action_values)\n",
    "        return action_probs  # Shape: (batch_size, self.max_teams)\n",
    "\n",
    "# Load your dataset\n",
    "data = train.groupby('DATE').filter(lambda x: (len(x) >= 4) and (len(x) <= 15))  # Adjusted to 15 games\n",
    "\n",
    "# Create the environment\n",
    "env = BettingEnv(data=data, max_games=15, K=0.05)  # Set threshold K as desired\n",
    "\n",
    "# Determine the device: 'cuda' if available, else 'cpu'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Training on device: {device}\")\n",
    "\n",
    "# Define the policy_kwargs with your custom policy network\n",
    "policy_kwargs = dict(\n",
    "    features_extractor_class=CustomPolicyNetwork,\n",
    "    features_extractor_kwargs=dict(\n",
    "        embedding_dim=128,\n",
    "        num_heads=4,\n",
    "        num_layers=2,\n",
    "    ),\n",
    "    # Set net_arch to an empty list because the output is handled in the features extractor\n",
    "    net_arch=[],\n",
    ")\n",
    "\n",
    "# Import PPO\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy, MultiInputActorCriticPolicy\n",
    "\n",
    "# Custom ActorCriticPolicy to use our CustomPolicyNetwork\n",
    "class CustomActorCriticPolicy(MultiInputActorCriticPolicy):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(CustomActorCriticPolicy, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def _build_mlp_extractor(self):\n",
    "        # The policy network output is already the action probabilities\n",
    "        pass  # No need to build additional networks\n",
    "\n",
    "# Instantiate the PPO agent\n",
    "model = PPO(\n",
    "    policy=CustomActorCriticPolicy,\n",
    "    env=env,\n",
    "    learning_rate=0.0003,\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    verbose=1,\n",
    "    tensorboard_log=\"./ppo_betting_tensorboard/\",\n",
    "    seed=SEED,\n",
    "    device=device  # Specify the device here\n",
    ")\n",
    "\n",
    "# Train the agent\n",
    "model.learn(total_timesteps=250000)\n",
    "\n",
    "# Save the trained model\n",
    "model.save('ppo_betting_agent')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
